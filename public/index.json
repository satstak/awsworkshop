[
{
	"uri": "/",
	"title": "AWS Security Roadshow Japan 2020",
	"tags": [],
	"description": "",
	"content": "ようこそ AWS Security Roadshow Japan 2020 ご参加頂きありがとうございます！このイベントはセキュリティ、アイデンティティ、コンプライアンスについて学ぶと共に、コミュニティーを形成するためのものです。 1日のイベントの中で、基調講演、セッション、Security JAMに参加してAWSサービスのエコシステムについて理解するとともに、AWSで実行されているワーウロードのセキュリティレベルを向上するためのベストプラクティスについて学習出来ます。\nセキュリティとコンプライアンスはAWSとお客様の間の責任共有モデル(https://aws.amazon.com/jp/compliance/shared-responsibility-model/)が基本となります。 AWS is responsible for protecting the infrastructure which runs all of the services offered and this responsibility is known as the Security of the Cloud. AWS customers benefit from a data center and network architecture built to satisfy the requirements of the most security-sensitive organizations. Customers responsibility, known as the Security in the Cloud, is determined by which services the customer chooses to use.\nThe workshops and other hands-on content we will discuss are focused on the Security in the Cloud and they will guide you through prepared scenarios that represent common use cases and operational tasks you\u0026rsquo;ll face in building securely on AWS. They will also highlight the design principals from the AWS Well-Architected Framework Security Pillar, which can help you improve your security posture. Additional frameworks that will be discussed include the NIST Cyber Security Framework and the AWS Cloud Adoption Framework Security Perspective. Having an understanding of these frameworks will help you in adopting the practices discussed in this AWS Cloud Security Virtual Event within your organizations.\nアジェンダ  基調講演 On-Demand Tracks  "
},
{
	"uri": "/workshops/module1/scenario/",
	"title": "1. Overview and Setup",
	"tags": [],
	"description": "",
	"content": "In today\u0026rsquo;s world of modern application development and immutable infrastructure deployed as code, there remains a number of reasons why engineers require shell-level access to their instances on occasion. They might need to kill runaway processes, debug problems on a live machine, or fine-tune configurations during development; all while maintaining a strong security profile.\nIn this session, you will configure AWS Systems Manager Session Manager to provide secure interactive access to your managed instances without the need to expose inbound ports, manage bastion hosts, or manage SSH keys. You will learn how Session Manager works by default and will progressively increase the security posture of your environment by enabling enhanced session encryption, configuring session logging and reducing default permissions.\nScenario You have been tasked with replacing the legacy bastion infrastructure at your organization with an alternative interactive shell-level access solution. You have been given a few key requirements and must develop a proof of concept that demonstrates the ability of Session Manager to address each:\n Secure Access: The solution must communicate over a secure encrypted channel for all control and session data. The solution must not require inbound ports to be authorized (e.g. TCP 22 or TCP 3389). Access Control: Users must be able to authenticate using IAM security principals (e.g. users and roles) and must not be required to leverage host-level authentication methods (e.g. public-key, password, etc.). Auditing: All session activity must be tracked and logged to include all command input and output. Cross-Platform Interactivity: The solution should provide synchronous execution of commands across both Windows and Linux platforms  Environment Setup   Click here if you\u0026#39;re not at an AWS event or are using your own account   In order to complete these workshops, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\nCreate an admin user\nIf you don\u0026rsquo;t already have an AWS IAM user with admin permissions, please use the following instructions to create one:\n Browse to the AWS IAM console. Click Users on the left navigation and then click Add User. Enter a User Name, check the checkbox for AWS Management Console access, enter a Custom Password, and click Next:Permissions. Click Attach existing policies directly, click the checkbox next to the AdministratorAccess, and click Next:review. Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.\u0026rdquo;  To setup the workshop environment, launch the CloudFormation stack below in the ap-southeast-2 AWS region using the \u0026ldquo;Deploy to AWS\u0026rdquo; links below. This will automatically take you to the console to run the template. In order to complete these workshops, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\n   Lab Template Region     Lab 1 - Eliminate bastion hosts with AWS Systems Manager Session Manager  AP Southeast 2 (Sydney)      Click Next on the Specify Template section.\n  Click Next on the Specify stack details section\n  Click Next on the Configure stack options section.\n  Finally, acknowledge that the template will create IAM roles under Capabilities and click Create.\n  This will bring you back to the CloudFormation console. You can refresh the page to see the stack starting to create. Before moving on, make sure the stack is in a CREATE_COMPLETE status.\n    Click here if you are at an AWS event where the Event Engine is being used   Confirm the CFN template has been deployed Browse to AWS CloudFormation Console You should see the stacks as displayed below deployed in your account. Look for the Stack that has the description \u0026ldquo;Session Manager Workshop\u0026rdquo;\nConfirm it has been deployed successfully, Status should be \u0026ldquo;CREATE_COMPLETE\u0026rdquo;, if not reach out to the support team for help.\n  Configure Lab Install Session Manager CLI Plugin into your Cloud9 session and create a KeyPair  Browse to the AWS Cloud9 Console Select Open IDE as shown below.  It may take a few seconds to initiate if the Cloud9 instance is not running already, once ready it should present you with a bash prompt in the terminal.\n  Within the terminal run the following command to download the Session Manager plugin RPM package\n curl \u0026quot;https://s3.amazonaws.com/session-manager-downloads/plugin/latest/linux_64bit/session-manager-plugin.rpm\u0026quot; -o \u0026quot;session-manager-plugin.rpm\u0026quot;  You should see an output as shown below.   Now run the install command:\n sudo yum install -y session-manager-plugin.rpm  You should an output as shown below.\n   Create a key pair for SSH access step of the lab:\n aws ec2 create-key-pair --key-name MyKeyPair --query 'KeyMaterial' --output text \u0026gt; MyKeyPair.pem  You should an output as shown below.\n   Update the permission on the private key file:\n chmod 400 MyKeyPair.pem  You should an output as shown below.\n  Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module3/scenario/",
	"title": "1. Scenario",
	"tags": [],
	"description": "",
	"content": "Overview In this workshop, you will have an environment provisioned consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them.\nScenario Welcome to Widgets LLC! You have just joined the team and your first task is to enhance security for the company website. The site runs on Linux, PHP and Apache and uses an EC2 instance and autoscaling group behind an Application Load Balancer (ALB). After an initial architecture assessment you have found multiple vulnerabilities and configuration issues. The dev team is swamped and will not be able to remediate code level issues for several weeks. Your mission in this workshop round is to build an effective set of controls that mitigate common attack vectors against web applications, and provide you with the monitoring capabilities needed to react to emerging threats when they occur.\n Level: Intermediate - Advanced Duration: 2 hours  Workshop Architecture You can now proceed to the Assess Phase.\n"
},
{
	"uri": "/agenda/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "The AWS Cloud Security Virtual event aims to improve your cloud security skills through a series of guided workshops, Security JAM scenario challenges and on-demand talks. In this portal you will find the following:\n Workshops On-Demand Tracks  Workshops These workshops are designed to help you get familiar with AWS Security and Operational services so you can improve your security and compliance objectives. You\u0026rsquo;ll be working with services such as AWS Systems Manager (operational management), AWS Config (configuration change management), Amazon Inspector (vulnerability \u0026amp; behavior analysis) and AWS WAF (web application firewall).\nAt the start of each workshop you will be able to watch a short video that will provide you an overview and a real world example.\n   Topic Description Skill Level Time     Eliminate Bastion Hosts with Systems Manager In this session, you will configure AWS Systems Manager Session Manager to provide secure interactive access to your managed instances without the need to expose inbound ports, manage bastion hosts, or manage SSH keys. You will learn how Session Manager works by default and will progressively increase the security posture of your environment by enabling enhanced session encryption, configuring session logging and reducing default permissions. Beginner - Intermediate 1 - 2 hours   Security Through Good Governance In this session, you will leverage Systems Manager and AWS Config to enforce governance across your AWS resources. You will collect inventory from your instances, automate patch management, ensuring consistency across your instances and automating compliance enforcement. Beginner - Intermediate 1 - 2 hours   Protecting Workloads from the Instance to the Edge In this session, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them. Advanced 2 - 3 hours   AWS Secrets Manager with Amazon RDS and AWS Fargate In this session, you will access the RDS database with Secrets Manager. You will then use Secrets Manager to rotate the data base password. You will then use Secrets Manager to access the database again to show that you can continue to access the data base after the rotation. In the second phase of the lab, you will extend your use of Secrets Manager into an AWS Fargate container. You will create an Amazon ECS task definition to pass secrets to the Fargate container and then launch the Fargate container. You will then SSH into the container to show that the secret was passed to the container and that you can access the RDS data base. Intermediate - Advanced 1 hour    On-Demand Tracks Watch great talks about AWS cloud security, presented by a mix of AWS customers and AWS experts.\n   Topic Description Skill Level     Introduction to AWS Security Ensuring security and compliance is a shared responsibility between AWS and the customer. In this session, we introduce the AWS Shared Responsibility Model along with key security services that allow you to build security controls that are aligned to the NIST Cybersecurity Framework categories: identify, protect, detect, respond, and recover. You also hear from a financial institution in Singapore about how they are developing a cloud security strategy that allows innovation within defined risk guardrails. 100   Advanced container security Learn how to leverage the identity and authorisation, network security and secrets management features of the wider AWS platform for their containers, including Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Container Service for Kubernetes (Amazon EKS). We also discuss best practices for the security of your container images such as scanning them for known vulnerabilities. 300   Cloud security for everyone: Multi-account strategy The cloud enables every business to have enterprise-grade security. Leveraging multiple accounts is an essential security pattern, even in small teams without dedicated security personnel. In this session, we dive deep into the accounts and learn how to configure them. Attendees are expected to have an understanding of the shared responsibility model and IAM. 300   Cloud-enabled security evolution with Origin Energy Moving your business to the cloud is a once-in-a-generation opportunity to significantly evolve your security capability and culture. Origin Energy, Australia’s largest energy retailer, started its cloud journey a few years ago. In this session, Origin Energy’s chief security officer and its security lead for cloud discuss their experience transforming a largely outsourced security capability into an in-house, business-aligned team. Learn how the company builds and runs cloud-native security at scale, at low cost, and with improved security. 200   Federated access and authorisation made simple In this session, learn how to implement attribute-based access control with role-based access control. We discuss how you can use this strategy to ensure that people have the right access to the things they need in their role, and we show you how to simplify their IAM policies in the process. Also learn how automation can deliver the consistency of access and authorisation, and how you can apply this to your environment. 200   How AFL secures real-time player tracking with encryption Through the sharing of real-time data and insights about the prevailing game and players, fan engagement in sports has been revolutionised. However, the sensitivity, influence, and impact of such data, as determined by various entities in the sports ecosystem, is critical. In this session, discover how a highly secure application has been designed and implemented not only to appease the various sporting entities, but also to ensure data is kept secure. 300   How to put SecOps to work in your organisation Open Universities Australia (OUA) migrated their core business systems to AWS in 2014 and have continued to optimise their environment on AWS. Leveraging AWS tools, OUA have automated responses to security events, limiting intervention of engineering staff, and enabling secure self-service tasks to simplify access to secure systems. In this session, OUA covers what worked, what didn\u0026rsquo;t, and what they learned along the way. 200   How Xinja built a neobank on the cloud Xinja is a 100-percent digital cloud-based neobank composed of a microservices architecture built with Kubernetes and Apache Kafka on AWS and hooked into many modern, cloud-based banking, payment, and channel platforms. This session focuses on how Xinja built its technology stack to exceed stringent security, risk, and resiliency requirements. Learn how it established a contemporary cloud network foundation, delivered transaction and deposit accounts with debit card payment capability, and integrated Apple Pay and Google Pay (including PCI DSS compliance). Additionally, hear how Xinja created multiple on-demand data pipelines and worked with APRA to secure its banking license and revolutionise its customers’ banking experience. 300   The fundamentals of AWS Security AWS offers an ever-growing landscape of services designed for a wide range of workloads in the cloud. But how do you secure all those different types of workloads? This session, intended for security-minded builders, introduces the fundamental AWS security building blocks that can be simply, easily, and authoritatively applied to anything you build on AWS. 200   IAM: Best practices for managing identity with AWS AWS Identity and Access Management (IAM) enables you to securely manage access to AWS services and resources. Using IAM, you can create and manage AWS users and groups, as well as use permissions to allow and deny their access to AWS resources. In this session, you learn best practices for managing user identity and permissions with AWS. We examine role-based access control (RBAC) and attribute-based access control (ABAC) models to ensure that people have the right access to what they need to perform their roles. 200   Security best practices: The Well-Architected way As you continually evolve your use of AWS, it’s important to consider ways to improve your security posture and take advantage of new security services and features. In this session, you explore architectural patterns for meeting common challenges, learn about service limits, and hear some tips and tricks, as well as learn ways to continually evaluate your architecture against best practices. Automation and tools are featured throughout, and we also include code giveaways! 200    "
},
{
	"uri": "/workshops/module3/perimeter-assess/",
	"title": "2. Perimeter Layer - Assess",
	"tags": [],
	"description": "",
	"content": "Environment Setup   Click here if you\u0026#39;re not at an AWS event or are using your own account   In order to complete these workshops, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\nCreate an admin user\nIf you don\u0026rsquo;t already have an AWS IAM user with admin permissions, please use the following instructions to create one:\n Browse to the AWS IAM console. Click Users on the left navigation and then click Add User. Enter a User Name, check the checkbox for AWS Management Console access, enter a Custom Password, and click Next:Permissions. Click Attach existing policies directly, click the checkbox next to the AdministratorAccess, and click Next:review. Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.\u0026rdquo;  To setup the workshop environment, launch the CloudFormation stack below in the ap-southeast-2 AWS region using the \u0026ldquo;Deploy to AWS\u0026rdquo; links below. This will automatically take you to the console to run the template. In order to complete these workshops, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\n   Lab Template Region     Lab 3 - Protecting Workloads from the Instance to the Edge  AP Southeast 2 (Sydney)      Click Next on the Specify Template section.\n  On the Specify stack details step, update the following parameters depending on how you are doing this workshop: info \u0026ldquo;Individual or an event not sponsored by AWS\u0026rdquo;\n  If you are sharing an AWS account with someone else in the same region, change the name of the stack to pww-yourinitials\n  Automated Scanner: Set to false.\n  Scanner Username: Enter null\n  Scanner Password: Enter null\n  Trusted Network CIDR: Enter a trusted IP or CIDR range you will access the site from using a web browser. You can obtain your current IP at Ifconfig.co The entry should follow CIDR notation. i.e. 10.10.10.10/32 for a single host.\n  Keep the defaults for the rest of the parameters.\n  Click Next\n  Click Next on the Configure stack options section.\n  Finally, acknowledge that the template will create IAM roles under Capabilities and click Create.\n  This will bring you back to the CloudFormation console. You can refresh the page to see the stack starting to create. Before moving on, make sure the stack is in a CREATE_COMPLETE status.\n    Click here if you are at an AWS event where the Event Engine is being used   Identify the stack that has been provisioned successfully  In the CloudFormation console you should see a list of stacks similar to the figure below. Select the stack with the description of \u0026ldquo;WAF Workshop Demo\u0026rdquo;    Look up the Stack Outputs   Go to the stack outputs and look for the website URL stored in the albEndpoint output value. Test access to the site by right clicking and opening in a new tab. Note the URL for your site as this will be used throughout this workshop.\n  While in the stack outputs, note the UniqueId value. This Id value will be used to identify the posture of your site within the automated scanner and the associated dashboard.\n  While still in stack outputs, right click the link in RedTeamHostSession and open in a new tab. This will launch an AWS Systems Manager Session Manager to the host you will use to perform manual scans against your site URL.\n  Website Scanning Environment and Tools In order to test your AWS WAF ruleset, this lab has been configured with two scanning capabilities; a Red Team Host where you can invoke manual scanning and an automated scanner which runs from outside your lab environment.\nThe scanner performs 10 basic tests designed to help simulate and mitigate common web attack vectors.\n Canary GET - Should not be blocked Canary POST - Should not be blocked SQL Injection (SQLi) in Query String SQL Injection (SQLi) in Cookie Cross Site Scripting (XSS) in Query String Cross Site Scripting (XSS) in Body Inclusion in Modules Cross Site Request Forgery (CSRF) Token Missing Cross Site Request Forgery (CSRF) Token Invalid Path Traversal  These basic tests are designed to provide common examples you can use to test AWS WAF functionality. You should perform thorough analysis and testing when implementing rules into your production environments.\n Website Scanning Environment and Tools - Manual Scanning Once you have started a Session Manager connection to your Red Team Host, the scanner script can be invoked by typing the following command:\nrunscanner\nThe scanner script will run each of the tests above and report back the following information:\n Request: The HTTP request command used. Test Name: The name of the test from list above. Result: The HTTP status code returned.  The logic in the scanner script color codes the response as follows:\n Green: 403 - Forbidden (Except for canary GET and POST tests.) Red: 200 - OK Blue: 404 - Not Found Yellow: 500 - Internal Server Error  About Scanner Tests and Colors: The color coding of the tests is provided to help to quickly assess the behavior of your WAF rules against their intended behavior. The goal is to achieve green color responses for all the tests. The purpose of the canary GET and POST requests are to ensure you have not unintentionally blocked legitimate traffic to your test site. These two tests should always return a 200 - OK response.\n What are the results of running the scanner script? Were the simulated malicious requests blocked? As you can see by running the script there are several vulnerabilities that need to be addressed. In the remediate phase you will configure an AWS WAF Web ACL to block these requests. When AWS WAF blocks a web request based on the conditions that you specify, it returns HTTP status code 403 (Forbidden). For a full view of the request and response information, you can paste the Request command directly into the console and add the \u0026ndash;debug argument.\nThe scanner script uses an open source HTTP client called httpie (https://httpie.org/). HTTPie—aitch-tee-tee-pie—is a command line HTTP client with an intuitive UI, JSON support, syntax highlighting, wget-like downloads, plugins, and more.\nWebsite Scanning Environment and Tools - Automated Scanning In addition to the manual scanning, automated scanning is also performed against your lab website. The automated tests are similar to the manual tests but the results are posted to a centralized scanning results dashboard along with the other workshop participants. You can identify the scanning results for your site using the UniqueId found in the CloudFormation outputs.\nYou can now proceed to the Remediate Phase.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/",
	"title": "基調講演",
	"tags": [],
	"description": "",
	"content": "Workshops These workshops are designed to help you get familiar with AWS Security and Operational services so you can improve your security and compliance objectives. You\u0026rsquo;ll be working with services such as AWS Systems Manager (operational management), AWS Config (configuration change management), Amazon Inspector (vulnerability \u0026amp; behavior analysis) and AWS WAF (web application firewall).\nAt the start of each workshop you will be able to watch a short video that will provide you an overview and a real world example.\n   Topic Description Skill Level Time     Eliminate Bastion Hosts with Systems Manager In this session, you will configure AWS Systems Manager Session Manager to provide secure interactive access to your managed instances without the need to expose inbound ports, manage bastion hosts, or manage SSH keys. You will learn how Session Manager works by default and will progressively increase the security posture of your environment by enabling enhanced session encryption, configuring session logging and reducing default permissions. Beginner - Intermediate 1 - 2 hours   Security Through Good Governance In this session, you will leverage Systems Manager and AWS Config to enforce governance across your AWS resources. You will collect inventory from your instances, automate patch management, ensuring consistency across your instances and automating compliance enforcement. Beginner - Intermediate 1 - 2 hours   Protecting Workloads from the Instance to the Edge In this session, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them. Advanced 2 - 3 hours   AWS Secrets Manager with Amazon RDS and AWS Fargate In this session, you will access the RDS database with Secrets Manager. You will then use Secrets Manager to rotate the data base password. You will then use Secrets Manager to access the database again to show that you can continue to access the data base after the rotation. In the second phase of the lab, you will extend your use of Secrets Manager into an AWS Fargate container. You will create an Amazon ECS task definition to pass secrets to the Fargate container and then launch the Fargate container. You will then SSH into the container to show that the secret was passed to the container and that you can access the RDS data base. Intermediate - Advanced 1 hour    "
},
{
	"uri": "/workshops/module3/perimeter-remediate/",
	"title": "3. Perimeter Layer - Remediate",
	"tags": [],
	"description": "",
	"content": "Mitigating Common Web Application Attack Vectors Using AWS WAF In the previous Build Phase, you identified several vulnerabilities in your web application. You are now going to design and implement an AWS WAF ruleset to help mitigate these vulnerabilities. In this section you will do the following tasks:\n Identify the WAF ACL for your site AWS WAF Rule design and considerations Console Walkthrough - Creating a Rule WAF Rule Creation and Solutions  Please ensure you are using the improved AWS WAF console and API experience for this workshop.\n Identify the WAF ACL for your Site   If needed, go to https://console.aws.amazon.com/console/home. You will be redirected to the AWS Management Console dashboard on successful login:   From the Management Console dashboard, navigate to the AWS WAF \u0026amp; Shield service console. You can do that several ways:\n Type \u0026ldquo;waf\u0026rdquo; in the AWS services panel search box and select the resulting option Expand the Services drop down menu (top left on the menu bar) and choose WAF \u0026amp; Shield Expand the All services area of the AWS services panel and choose WAF \u0026amp; Shield Once selected, you will be redirected to the AWS WAF \u0026amp; AWS Shield service console. You may see an initial landing page at first. Choose Go to AWS WAF:    In the side bar menu on the left, pick the Web ACLs option under the AWS WAF heading. If the list of Web ACLs appears empty, select (Asia Pacific) Sydney region.  Click on the WAF Web ACL Name to select the existing Web ACL. Once the detail pane is loaded on the left of your screen, you will see three tabs as shown in the screenshot. Toggle to Rules:  Validate that you are able to see a pre-existing rule, configured to block requests, and that your Web ACL is associated with an Application load balancer resource. You can drill down further into the properties of the existing rule, by selecting the rule name and clicking Edit. This rule references IP sets for the loopback/localhost IP addresses (127.0.0.0/8, ::1/128).  Viewing and Logging Requests: In the Overview tab for your Web ACL, you can view a sample of the requests that have been inspected by the WAF. For each sampled request, you can view detailed data about the request, such as the originating IP address and the headers included in the request. You also can view which rule the request matched, and whether the rule is configured to allow or block requests. You can refer to the sampled requests throughout this exercise to monitor activity and look for suspicious activity. Although not used in this workshop, in the Logging and metrics tab, you can enable full logging to get detailed information about traffic that is analyzed by your web ACL.\n AWS WAF Rule Design and Considerations Basics You use AWS WAF to control how an Amazon CloudFront distribution, an Amazon API Gateway API, or an Application Load Balancer responds to web requests.\nWeb ACLs \u0026ndash; You use a web access control list (ACL) to protect a set of AWS resources. You create a web ACL and define its protection strategy by adding rules. Rules define criteria for inspecting web requests and specify how to handle requests that match the criteria. You set a default action for the web ACL that indicates whether to block or allow through those requests that pass the rules inspections.\nRules \u0026ndash; Each rule contains a statement that defines the inspection criteria, and an action to take if a web request meets the criteria. When a web request meets the criteria, that's a match. You can use rules to block matching requests or to allow matching requests through. You can also use rules just to count matching requests.\nRules groups \u0026ndash; You can use rules individually or in reusable rule groups. AWS Managed Rules and AWS Marketplace sellers provide managed rule groups for your use. You can also define your own rule groups.\nRule statements are the part of a rule that tells AWS WAF how to inspect a web request. When AWS WAF finds the inspection criteria in a web request, we say that the web request matches the statement. Every rule statement specifies what to look for and how, according to the statement type.\nEvery rule in AWS WAF has a single top-level rule statement, which can contain other statements. Rule statements can be very simple. For example, you could have a statement that provides just a set of originating countries to check your web requests for. Rule statements can also be very complex. For example, you could have a statement that combines many other statements with logical AND, OR, and NOT statements.\nHow WAF Works After you create your web ACL, you can associate it with one or more AWS resources. The resource types that you can protect using AWS WAF web ACLs are Amazon CloudFront distributions, Amazon API Gateway APIs, and Application Load Balancers.\nIn order to illustrate the process of creating WAF rules, we will walk through the creation of the first rule in your WAF ACL.\n Rule Design Considerations: Use the following guiding questions when planning WAF rules:\n What is the intended purpose of the rule? What HTTP request components apply to the purpose of the rule? Do you already have rules or rule groups targeting those request components that you can expand? Is that desirable? How can you define the purpose of the rule in a Boolean logic expression? Will the rule require nested statements under logical AND or OR rule statements? Are any transformations relevant to my input content type?  Sample Rule purpose:  Detect SQL Injection in query string, use \u0026lsquo;block\u0026rsquo; action in Web ACL  HTTP request components:  Request Method \u0026ndash; form input typically gets submitted using a GET HTTP request method Query String \u0026ndash; the SQL injection attempt is located in the query string  Define the purpose of the rule using Boolean logic:  If Query String contains suspected SQL Injection then block  Sample Rule - Statement to implement:  Contains SQL injection attacks Match type targeting the request Query string  Relevant transformations:  Contains SQL injection attacks Match type query string is URL encoded, so we will apply the URL_DECODE transformation.  Rules to implement:  Rule with 1 predicate Contains SQL injection attacks Match type  Console Walkthrough - Creating a Rule   In the AWS WAF console, create a SQL injection rule by clicking the Web ACL, Add rules, Add my own rules and rule groups\n  Click on Rule builder, provide matchSQLi for the Name and keep Regular rule for Type:\n  For If a request select matches the statement. Under Statement, for Inspect select Query string, for Match type select Contains SQL injection attacks, for Text transformation select URL decode and for Action select Block.   Click on Add Rule, select Save on the Set rule priority page.\n View the matchSQLi rule in the Rule visual editor to confirm it is correct.    For a more comprehensive discussion of common vulnerabilities for web applications, as well as how to mitigate them using AWS WAF, and other AWS services, please refer to the Use AWS WAF to Mitigate OWASP\u0026rsquo;s Top 10 Web Application Vulnerabilities whitepaper.\n WAF Rule Creation and Solutions In this phase, we will have a set of 6 exercises walking you through the process of building a basic mitigation rule set for common vulnerabilities. We will build these rules from scratch, so you can gain familiarity with the AWS WAF programming model and you can then write rules specific to your applications.\nFor the exercises below, you will find the high level description and solution configuration for your web ACL. You can test your ACL ruleset at any time using the Red Team Host. For AWS sponsored event, you can also view test results on the WAF Lab Dashboard.\n 1. SQL Injection \u0026amp; Cross Site Scripting Mitigation Use the SQL injection, cross-site scripting, as well as string and regex matching to build rules that mitigate injection attacks and cross site scripting attacks.\nConsider the following:\n How does your web application accept end-user input (whether directly or indirectly). Which HTTP request components does that input get inserted into? What kind of content encoding considerations do you need to factor in for the input format? What considerations do you need to account for in regards to false positives? For example, does your application legitimately need to accept SQL statements as input?  How do the requirements derived from the above questions affect your solution?\nSQL Injection Solution  Update the matchSQLi rule with 2 additional statements  Select the matchSQLi rule and click Edit (You should have created this rule in the walk through above) Change If a request to matches at least one of the statements (OR) Click Add another statement: body, contains sql injection attacks, html entity decode and URL decode Click Add another statement: header, cookie (type manually), contains sql injection attacks, url decode   Select Save and Save again on the Set rule priority page. Re-run the WAF test script (runscanner) from your red team host to confirm requests are blocked  Thus far you have used the Visual Rule Editor to create WAF Rules but every web ACL also has a JSON format representation. In there, you see these special types of rule statements. For rules of any complexity, managing your web ACL using the JSON editor is the easiest way to go. You can retrieve the complete configuration for a web ACL in JSON format, modify it as you need, and then provide it through the console, API, or CLI. AWS WAF supports nesting of rule statements. To combine rule statement results, you nest the statements under logical AND or OR rule statements. The visual editor on the console supports one level of rule statement nesting, which works for many needs. To nest more levels, you can edit the JSON representation of the rule on the console.\n Cross Site Scripting Solution   Create a new rule named matchXSS and for If a request choose matches at least one of the statements (OR). Add statements:\n all query parameters, contains xss injection attacks, url decode body, contains xss injection attacks, html entity decode and url decode header, cookie (type manually), contains xss injection attacks, url decode Click on Add Rule and then select Save on the Set rule priority page.    Edit the rule, click the Rule JSON editor and note the structure and syntax of the rule logic.\n  Add an exception statement for the XSS rule to allow access to /reportBuilder/Editor.aspx. Note that we are using the JSON editor here due to the nested logic required for the exception.\n After reviewing it, clear the existing editor content for the matchXSS rule and paste the following JSON Nested Statement with XSS Exception Solution    { \u0026quot;Name\u0026quot;: \u0026quot;matchXSS\u0026quot;, \u0026quot;Priority\u0026quot;: 2, \u0026quot;Action\u0026quot;: { \u0026quot;Block\u0026quot;: {} }, \u0026quot;VisibilityConfig\u0026quot;: { \u0026quot;SampledRequestsEnabled\u0026quot;: true, \u0026quot;CloudWatchMetricsEnabled\u0026quot;: true, \u0026quot;MetricName\u0026quot;: \u0026quot;matchXSS\u0026quot; }, \u0026quot;Statement\u0026quot;: { \u0026quot;AndStatement\u0026quot;: { \u0026quot;Statements\u0026quot;: [{ \u0026quot;NotStatement\u0026quot;: { \u0026quot;Statement\u0026quot;: { \u0026quot;ByteMatchStatement\u0026quot;: { \u0026quot;SearchString\u0026quot;: \u0026quot;/reportBuilder/Editor.aspx\u0026quot;, \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;UriPath\u0026quot;: {} }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;NONE\u0026quot; }], \u0026quot;PositionalConstraint\u0026quot;: \u0026quot;STARTS_WITH\u0026quot; } } } }, { \u0026quot;OrStatement\u0026quot;: { \u0026quot;Statements\u0026quot;: [{ \u0026quot;XssMatchStatement\u0026quot;: { \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;QueryString\u0026quot;: {} }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;URL_DECODE\u0026quot; }] } }, { \u0026quot;XssMatchStatement\u0026quot;: { \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;Body\u0026quot;: {} }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;HTML_ENTITY_DECODE\u0026quot; }, { \u0026quot;Priority\u0026quot;: 1, \u0026quot;Type\u0026quot;: \u0026quot;URL_DECODE\u0026quot; } ] } }, { \u0026quot;XssMatchStatement\u0026quot;: { \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;SingleHeader\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;cookie\u0026quot; } }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;URL_DECODE\u0026quot; }] } } ] } } ] } } }  Click Save rule\n  Re-run the WAF test script (runscanner) from your red team host to confirm requests are blocked\n  If your Systems Manager Session Manager session times out, create a new session.\n 2. Mitigate File Inclusion \u0026amp; Path Traversal Use the string and regex matching to build rules that block specific patterns indicative of unwanted path traversal or file inclusion.\nConsider the following:  Can end users browse the directory structure of your web folders? Do you have directory indexes enabled? Does your application (or any dependency components) use input parameters in filesystem or remote URL references? Do you adequately lock down access so input paths cannot be manipulated? What considerations do you need to account for in regards to false positives (directory traversal signature patterns)?  Build rules that ensure the relevant HTTP request components used for input into paths do not contain known path traversal patterns.\nSolution  Create a new rule named matchTraversal and for If a request choose matches at least one of the statements (OR). Add statements:  uri_path, starts with string, /include , url_decode query_string, contains string, ../ , url_decode query_string, contains string, :// , url_decode Click on Add Rule and then select Save on the Set rule priority page.   Re-run the WAF test script (runscanner) from your red team host to confirm requests are blocked  3. Enforce Request Hygiene Use the string and regex matching, size constraints and IP address matching to build rules that block non-conforming or low value HTTP requests.\nConsider the following:   Are there limits to the size of the various HTTP request components relevant to your web application? For example, does your application ever use URIs that are longer than 100 characters in size?\n  Are there specific HTTP request components without which your application cannot operate effectively (e.g. CSRF token header, authorization header, referrer header)?\n  Build rules that ensure the requests your application ends up processing are valid, conforming and valuable.\nSolution   In the left pane, choose Regex pattern sets, Create regex pattern set\n Regex pattern set name csrf, Regular expressions \\^\\[0-9a-f\\]{40}\\$  The Regex pattern above is a simple example that matches the string length (40) and characters (0-9 or a-f). Copy the Regex pattern set ID into a scratch file to refer to it later. Note your AWS account Id (in CloudFormation Stack Outputs) and region and add them to the scratch file.      Create a new rule and choose Rule JSON editor\n  Delete any existing text and paste the following JSON below. Review the statements in the JSON\n  Update the region, AWS account Id and Regex pattern ID with the one created in the previous step\n  Nested Statement with Request Hygiene Solution\n  { \u0026quot;Name\u0026quot;: \u0026quot;matchCSRF\u0026quot;, \u0026quot;Priority\u0026quot;: 3, \u0026quot;Action\u0026quot;: { \u0026quot;Block\u0026quot;: {} }, \u0026quot;VisibilityConfig\u0026quot;: { \u0026quot;SampledRequestsEnabled\u0026quot;: true, \u0026quot;CloudWatchMetricsEnabled\u0026quot;: true, \u0026quot;MetricName\u0026quot;: \u0026quot;matchCSRF\u0026quot; }, \u0026quot;Statement\u0026quot;: { \u0026quot;AndStatement\u0026quot;: { \u0026quot;Statements\u0026quot;: [{ \u0026quot;NotStatement\u0026quot;: { \u0026quot;Statement\u0026quot;: { \u0026quot;RegexPatternSetReferenceStatement\u0026quot;: { \u0026quot;ARN\u0026quot;: \u0026quot;arn:aws:wafv2:YOUR_REGION:ACCOUNT_ID:regional/regexpatternset/csrf/YOUR_REGEX_PATTERN_ID\u0026quot;, \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;SingleHeader\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;x-csrf-token\u0026quot; } }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;URL_DECODE\u0026quot; }] } } } }, { \u0026quot;OrStatement\u0026quot;: { \u0026quot;Statements\u0026quot;: [{ \u0026quot;ByteMatchStatement\u0026quot;: { \u0026quot;SearchString\u0026quot;: \u0026quot;/form.php\u0026quot;, \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;UriPath\u0026quot;: {} }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;NONE\u0026quot; }], \u0026quot;PositionalConstraint\u0026quot;: \u0026quot;STARTS_WITH\u0026quot; } }, { \u0026quot;ByteMatchStatement\u0026quot;: { \u0026quot;SearchString\u0026quot;: \u0026quot;/form.php\u0026quot;, \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;UriPath\u0026quot;: {} }, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Priority\u0026quot;: 0, \u0026quot;Type\u0026quot;: \u0026quot;NONE\u0026quot; }], \u0026quot;PositionalConstraint\u0026quot;: \u0026quot;EXACTLY\u0026quot; } } ] } } ] } } }  Click Save rule and then select Save on the Set rule priority page.\n  Re-run the WAF test script (runscanner) from your red team host to confirm requests are blocked\n  If you have 30 minutes or less remaining in the workshop, you should consider proceeding to the Host Layer round. There will be time during the Inspector Assessment run to continue the WAF excercises. The remaining exercises below are optional. You should proceed to the Verify Phase and come back to the content below if time permits.\n  4. Limit Attack Footprint (Optional) Use geo matching to build rules that limit the attack footprint against the exposed components of your application.\nConsider the following:\n  Does your web application have server-side include components in the public web path?\n  Does your web application have components at exposed paths that are not used (or dependencies have such functions)?\n  Do you have administrative, management, status or health check paths and components that aren\u0026rsquo;t meant for end user access?\n  You should consider blocking access to such elements, or limiting access to known sources, either whitelisted IP addresses or geographic locations.\nSolution   Create a new rule and choose Rule JSON editor\n Paste the following JSON  Nested Statement with Limit Attack Footprint Solution\n  { \u0026quot;Name\u0026quot;: \u0026quot;matchAdminNotAffiliate\u0026quot;, \u0026quot;Priority\u0026quot;: 4, \u0026quot;Action\u0026quot;: { \u0026quot;Block\u0026quot;: {} }, \u0026quot;VisibilityConfig\u0026quot;: { \u0026quot;SampledRequestsEnabled\u0026quot;: true, \u0026quot;CloudWatchMetricsEnabled\u0026quot;: true, \u0026quot;MetricName\u0026quot;: \u0026quot;matchAdminNotAffiliate\u0026quot; }, \u0026quot;Statement\u0026quot;: { \u0026quot;AndStatement\u0026quot;: { \u0026quot;Statements\u0026quot;: [{ \u0026quot;NotStatement\u0026quot;: { \u0026quot;Statement\u0026quot;: { \u0026quot;GeoMatchStatement\u0026quot;: { \u0026quot;CountryCodes\u0026quot;: [ \u0026quot;US\u0026quot;, \u0026quot;RO\u0026quot; ] } } } }, { \u0026quot;ByteMatchStatement\u0026quot;: { \u0026quot;FieldToMatch\u0026quot;: { \u0026quot;UriPath\u0026quot;: {} }, \u0026quot;PositionalConstraint\u0026quot;: \u0026quot;STARTS_WITH\u0026quot;, \u0026quot;SearchString\u0026quot;: \u0026quot;/admin\u0026quot;, \u0026quot;TextTransformations\u0026quot;: [{ \u0026quot;Type\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;Priority\u0026quot;: 0 }] } } ] } } } Click on Add Rule and then click Save  5. Detect \u0026amp; Mitigate Anomalies (Optional) What constitutes an anomaly in regards to your web application? A few common anomaly patterns are:\n Unusually elevated volume of requests in general Unusually elevated volumes of requests to specific URI paths Unusually elevated levels of requests generating specific non-HTTP status 200 responses Unusually elevated volumes from certain sources (IPs, geographies) Usual request signatures (referrers, user agent strings, content types, etc)  Do you have mechanisms in place to detect such patterns? If so, can you build rules to mitigate them?\nSolution  Create a new rule named matchRateLogin of type Rate-based rule  Rate limit 1000 choose Source IP address choose Only consider requests that match the criteria in a rule statement choose If a request choose matches at least one of the statements (OR). Add statements:  uri_path, starts with string, /login.php http_method, exactly matches string, POST text transformation, None   Click on Add Rule and then click Save    6. Reputation Lists, Nuisance Requests (Optional) Reputation lists (whitelists or blacklists) are a good way to filter and stop servicing low value requests. This can reduce operating costs, and reduce exposure to attack vectors. Reputation lists can be self-maintained: lists of identifiable actors that you have determined are undesired. They can be identified any number of ways:\n the source IP address the user agent string reuse of hijacked authorization or session tokens, attempting to make requests to paths that clearly do not exist in your application but are well known vulnerable software packages (probing)  Build blacklists of such actors using the relevant statements and set up rules to match and block them. An example IP-based blacklist already exists in your sandbox environment.\nReputation lists can also be maintained by third parties. The AWS WAF Security Automations allow you to implement IP-based reputation lists.\nSolution  edit the IP set SampleIPSetV4  add a test IP address You can obtain your current IP at Ifconfig.co or CheckIP. The entry should follow CIDR notation. i.e. 10.10.10.10/32 for a single host.   Create a new rule named matchRepNuisance  uri_path, starts with, /phpmyadmin, no transform     You can now proceed to the Verify Phase.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/ondemandtracks/",
	"title": "録画されたセッション",
	"tags": [],
	"description": "",
	"content": "On-Demand Tracks Watch the following great talks about AWS cloud security, presented by a mix of AWS customers and AWS experts.\n   Topic Description Skill Level     Introduction to AWS Security Ensuring security and compliance is a shared responsibility between AWS and the customer. In this session, we introduce the AWS Shared Responsibility Model along with key security services that allow you to build security controls that are aligned to the NIST Cybersecurity Framework categories: identify, protect, detect, respond, and recover. You also hear from a financial institution in Singapore about how they are developing a cloud security strategy that allows innovation within defined risk guardrails. 100   Advanced container security Learn how to leverage the identity and authorisation, network security and secrets management features of the wider AWS platform for their containers, including Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Container Service for Kubernetes (Amazon EKS). We also discuss best practices for the security of your container images such as scanning them for known vulnerabilities. 300   Cloud security for everyone: Multi-account strategy The cloud enables every business to have enterprise-grade security. Leveraging multiple accounts is an essential security pattern, even in small teams without dedicated security personnel. In this session, we dive deep into the accounts and learn how to configure them. Attendees are expected to have an understanding of the shared responsibility model and IAM. 300   Cloud-enabled security evolution with Origin Energy Moving your business to the cloud is a once-in-a-generation opportunity to significantly evolve your security capability and culture. Origin Energy, Australia’s largest energy retailer, started its cloud journey a few years ago. In this session, Origin Energy’s chief security officer and its security lead for cloud discuss their experience transforming a largely outsourced security capability into an in-house, business-aligned team. Learn how the company builds and runs cloud-native security at scale, at low cost, and with improved security. 200   Federated access and authorisation made simple In this session, learn how to implement attribute-based access control with role-based access control. We discuss how you can use this strategy to ensure that people have the right access to the things they need in their role, and we show you how to simplify their IAM policies in the process. Also learn how automation can deliver the consistency of access and authorisation, and how you can apply this to your environment. 200   How AFL secures real-time player tracking with encryption Through the sharing of real-time data and insights about the prevailing game and players, fan engagement in sports has been revolutionised. However, the sensitivity, influence, and impact of such data, as determined by various entities in the sports ecosystem, is critical. In this session, discover how a highly secure application has been designed and implemented not only to appease the various sporting entities, but also to ensure data is kept secure. 300   How to put SecOps to work in your organisation Open Universities Australia (OUA) migrated their core business systems to AWS in 2014 and have continued to optimise their environment on AWS. Leveraging AWS tools, OUA have automated responses to security events, limiting intervention of engineering staff, and enabling secure self-service tasks to simplify access to secure systems. In this session, OUA covers what worked, what didn\u0026rsquo;t, and what they learned along the way. 200   How Xinja built a neobank on the cloud Xinja is a 100-percent digital cloud-based neobank composed of a microservices architecture built with Kubernetes and Apache Kafka on AWS and hooked into many modern, cloud-based banking, payment, and channel platforms. This session focuses on how Xinja built its technology stack to exceed stringent security, risk, and resiliency requirements. Learn how it established a contemporary cloud network foundation, delivered transaction and deposit accounts with debit card payment capability, and integrated Apple Pay and Google Pay (including PCI DSS compliance). Additionally, hear how Xinja created multiple on-demand data pipelines and worked with APRA to secure its banking license and revolutionise its customers’ banking experience. 300   The fundamentals of AWS Security AWS offers an ever-growing landscape of services designed for a wide range of workloads in the cloud. But how do you secure all those different types of workloads? This session, intended for security-minded builders, introduces the fundamental AWS security building blocks that can be simply, easily, and authoritatively applied to anything you build on AWS. 200   IAM: Best practices for managing identity with AWS AWS Identity and Access Management (IAM) enables you to securely manage access to AWS services and resources. Using IAM, you can create and manage AWS users and groups, as well as use permissions to allow and deny their access to AWS resources. In this session, you learn best practices for managing user identity and permissions with AWS. We examine role-based access control (RBAC) and attribute-based access control (ABAC) models to ensure that people have the right access to what they need to perform their roles. 200   Security best practices: The Well-Architected way As you continually evolve your use of AWS, it’s important to consider ways to improve your security posture and take advantage of new security services and features. In this session, you explore architectural patterns for meeting common challenges, learn about service limits, and hear some tips and tricks, as well as learn ways to continually evaluate your architecture against best practices. Automation and tools are featured throughout, and we also include code giveaways! 200    "
},
{
	"uri": "/workshops/module4/scenario/",
	"title": "1. Architecture",
	"tags": [],
	"description": "",
	"content": "Workshop architecture This environment consists of a VPC with two subnets. The subnets contains an Amazon EC2 bastion host running Amazon Linux 2 used for running AWS CLI commands and a private, not publicly-accessible, Amazon RDS MySQL database instance. The subnets are chosen based on the availability of an interface VPC endpoint for AWS Secrets Manager. The VPC also contains an AWS Fargate container running the Amazon Linux 2 operating system. In the RDS and Fargate phases of the workshop, you will learn how to use AWS Secrets Manager to access the RDS database from both a standard EC2 instance and the Fargate container.\nEnvironment Setup   Click here if you\u0026#39;re not at an AWS event or are using your own account   In order to complete these workshops, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\nCreate an admin user\nIf you don\u0026rsquo;t already have an AWS IAM user with admin permissions, please use the following instructions to create one:\n Browse to the AWS IAM console. Click Users on the left navigation and then click Add User. Enter a User Name, check the checkbox for AWS Management Console access, enter a Custom Password, and click Next:Permissions. Click Attach existing policies directly, click the checkbox next to the AdministratorAccess, and click Next:review. Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.\u0026rdquo;  To setup the workshop environment, launch the CloudFormation stack below in the ap-southeast-2 AWS region using the \u0026ldquo;Deploy to AWS\u0026rdquo; links below. This will automatically take you to the console to run the template. In order to complete these workshops, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\n   Lab Template Region     Lab 4 - AWS Secrets Manager with Amazon RDS and AWS Fargate  AP Southeast 2 (Sydney)      Click Next on the Specify Template section.\n  Click Next on the Specify stack details section\n  Click Next on the Configure stack options section.\n  Finally, acknowledge that the template will create IAM roles under Capabilities and click Create.\n  This will bring you back to the CloudFormation console. You can refresh the page to see the stack starting to create. Before moving on, make sure the stack is in a CREATE_COMPLETE status.\n    Click here if you are at an AWS event where the Event Engine is being used   Confirm the CFN template has been deployed Browse to AWS CloudFormation Console You should see the stacks as displayed below deployed in your account. Look for the Stack that has the description \u0026ldquo;Session Manager Workshop\u0026rdquo;\nConfirm it has been deployed successfully, Status should be \u0026ldquo;CREATE_COMPLETE\u0026rdquo;, if not reach out to the support team for help.\n  "
},
{
	"uri": "/workshops/module3/perimeter-verify/",
	"title": "4. Perimeter Layer - Verify",
	"tags": [],
	"description": "",
	"content": "Mitigating Common Web Application Attack Vectors Using AWS WAF In the previous remediation phase, you implemented an AWS WAF ruleset to protect your site from common attack vectors. You are now going to reassess the posture of the site to confirm the rules are performing as intended and blocking the simulated malicious requests.\n Confirm malicious requests are blocked by WAF policy Implement WAF monitoring dashboard using Amazon CloudWatch (Optional)  Confirm malicious requests are blocked by WAF policy If needed, start a Session Manager connection to your Red Team Host, the scanner script can be invoked by typing the following command:\nrunscanner  Confirm that all of the tests in the script pass. If requests (other than canary) are not being blocked, go back to the remediate phase and confirm your conditions and rules are properly configured.\nIf the the automated scanner is being used for your event (AWS sponsored), you should also see green for your unique Id on the WAF scanning results dashboard.\nImplement WAF monitoring dashboard using Amazon CloudWatch (Optional) Use CloudWatch Dashboards to create a monitoring system for your protection layer. The following AWS article details this process: https://aws.amazon.com/blogs/aws/cloudwatch-dashboards-create-use-customized-metrics-views/\nHere are some sample of metrics that you can use. Starting from top left side, in clockwise order, we have:\n  Allowed vs Blocked Requests: if you receive a surge in allowed access (2 times normal peak access) or blocked access (any period that identifies more than 1,000 blocked requests), you can configure CloudWatch to send an alert. The idea here is to track known DDoS (when blocked requests increase) or new version of attack (when the requests are allowed to access the system);\n  BytesDownloaded vs. Uploaded: help you identify when DDoS attack targets a service that doesn't need to receive a huge amount of access in order to exhaust resources (ex: search engine component sending MBs of information for one specific request parameters set);\n  ELB Spillover and Queue length: use these metrics to verify if the attack is already causing damage to the infrastructure and/or for some reason, the attacker is bypassing protection layer and attacking directly unprotected resources;\n  ELB Request Count: same as above, helps you identify damage by checking if the attacker is bypassing protection layer and/or CloudFront cache; review rules to increase cache hit rate;\n  ELB Healthy Host: another system health check metric;\n  ASG CPU Utilization: identify if the attacker is not only bypassing the CloudFront/WAF but also the ELB layer, also use to identify the damage impact of an attack;\n   This concludes the Perimeter Round. You can now proceed to the You can now proceed to the Host Layer Round.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module4/rds/",
	"title": "2. RDS with Secrets Manager",
	"tags": [],
	"description": "",
	"content": "Overview In this phase, you will learn how to use AWS Secrets Manager for rotating the password for a private Amazon RDS database. As a reminder, the environment provisioned by CloudFormation is shown in the figure below. You will not be working with the AWS Fargate container in this phase so it is not shown below.\nImportant For the sake of simplicity, this tutorial uses jq to parse the secret value into environment variables to allow for easy command line manipulation. This is NOT a security best practice for a production environment. In a production environment, we recommend that you don\u0026rsquo;t store passwords in environment variables. Click here to read about the best practices for using Secrets Manager.\nView the CloudFormation stack   Go to the CloudFormation console and identify the stack that you built. Make sure you are in the correct region. The list of stacks will look similar to the figure below. The appearance may vary based on the version of the console you are using. Your stack will have a description of Workshop for AWS Secrets Manager with Amazon RDS and AWS Fargate.   Click the stack name link to reveal more information about the stack as shown in the figure below.   Click the Outputs tab as shown in the figure above to list the outputs of the stack which will be similar to the figure below.   The meanings of the output values are described in the table below.\n   Key Meaning of Value     BastionIP The IP address of the bastion host   DBInstance The Amazon RDS instance id   DBPassword The master password of the RDS data base   DBUser The master user of the RDS data base   EC2UserPassword The password for the ec2-user id   ECRRepository The ECR repository id   ECSCluster The ECS cluster id    You will need the values in the DBInstance, DBPassword, DBUser, and EC2UserPassword outputs in the next section so copy them to a file on your desktop so they are readily available.\nStore the secret In this section, you will store the RDS database credentials in AWS Secrets Manager.\n Go to the Secrets Manager console. Check your region to make sure the Secrets Manager console is operating in the ap-southeast-2 region. Click Store a new secret. Select the Credentials for RDS database radio button. Copy the values for the DBUser and DBPassword CloudFormation output values that you got from the CloudFormation stack into the User name and Password fields respectively. For the encryption key, Secrets Manager gives you the option of using the default KMS key that Secrets Manager creates for your account. This default key is managed by Secrets Manager itself. It provides encryption but you do not have the ability manage the policies or grants for the key. You can also specify your own KMS key which gives you more the option of configuring grants and policies that provide more granular permissions. In a production environment that requires fine-grained security controls, you would likely choose your own key. For this workshop we do not require these additional controls so select DefaultEncryptionKey in the dropdown menu. Scroll down to the bottom of the page and you will see a list of your RDS instances. Select the RDS instance based on the DBInstance CloudFormation output value.  Click Next. Enter a name for the secret. Use smdemo as shown below.  Click Next. Select Disable automatic rotation and then click Next. We will enable rotation later in this module.  Click Store.  You have now stored your secret value as shown below. Access the database In this section, you will connect to the bastion host so you can run scripts that the CloudFormation template has created on the instance.\n  Connect to the bastion host using AWS Systems Manager Session Manager. To do this:\n Go to the Systems Manager console. Click Session Manager. Click Start session. Select the radio button for the instance called smdemo-host. Click Start session.    The scripts you will be using are owned by the ec2-user account. Enter the command below to change your effective user id and directory to those of ec2-user:\n  sudo su - ec2-user  Display the current directory using this command:  ls You will see two shell scripts.\n mysql.oldway.sh - This shell script connects to the database the \u0026ldquo;old\u0026rdquo; way, using a hard-coded password. mysql.newway.sh - This shell script connects to the database the \u0026ldquo;new\u0026rdquo; way, using AWS Secrets Manager. View the file mysql.oldway.sh using this command:  cat mysql.oldway.sh You will see contents similar to the lines below. The values PASSWORD, USER, and ENDPOINT represent the hard-coded database password, username, and host endpoint.\n(Note: This code fragment is for illustration only and not intended for copying.) #/bin/bash # mysql.oldway.sh # This is the old way of accessing a database, with a hard-coded password. # This script will only work right after the CloudFormation template runs. # After you store and rotate the secret, you will need to use the # mysql.newway.sh script. mysql \\ -pPASSWORD \\ -u USER \\ -P 3306 \\ -h ENDPOINT  Test the script mysql.oldway.sh using the commands shown below. The first command invokes the script. The subsequent commands select the database, display the table names in the database, query the rows in the table, and exit MySQL. Note that MySQL commands end with a semicolon (\u0026quot;;\u0026quot;).  ./mysql.oldway.sh use smdemo; show tables; select * from bookinfo; quit; You can see an example of the output below. This shows that you can access the database, the \u0026ldquo;old\u0026rdquo; way, with a hard-coded user name and password. You may be wondering why MariaDB appears in the image below. Amazon Linux 2 includes the MariaDB port of the mysql command as an \u0026ldquo;extras\u0026rdquo; module. The mysql program is compatible with both MySQL and MariaDB.  View the file mysql.newway.sh by entering this command:  cat mysql.newway.sh Take a look at the lines below.\n(Note: This code fragment is for illustration and not intended for copying.) getsecretvalue() { aws secretsmanager get-secret-value --secret-id $1 | \\ jq .SecretString | \\ jq fromjson } The above lines define a shell function that uses the AWS CLI to retrieve the secret whose name is passed as a command line argument ($1). The result is a JSON string so the jq utility is used to extract the actual value of the secret whose JSON key is named SecretString. Here is an example of what a SecretString looks like:\n(Note: This code fragment is for illustration and not intended for copying.) { \u0026quot;engine\u0026quot;: \u0026quot;mysql\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;myuser\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;mypassword\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;my-database-endpoint.ap-southeast-2.rds.amazonaws.com\u0026quot;, \u0026quot;dbname\u0026quot;: \u0026quot;myDatabase\u0026quot;, \u0026quot;port\u0026quot;: \u0026quot;3306\u0026quot; } Note that the SecretString itself is a JSON structure. Now look at the following lines.\n(Note: This code fragment is for illustration and not intended for copying.) secret=`getsecretvalue $1` user=$(echo $secret | jq -r .username) password=$(echo $secret | jq -r .password) endpoint=$(echo $secret | jq -r .host) port=$(echo $secret | jq -r .port) These lines call the shell function and then use jq to extract the database username, password, endpoint, and port from the SecretString. These are then passed to the mysql command as shown on the lines below. Note that there is no space after the -p option.\n(Note: This code fragment is for illustration and not intended for copying.) mysql \\ -p$password \\ -u $user \\ -P $port \\ -h $endpoint  Run the mysql.newway.sh script by running the commands below. The first command invokes the script. Note that you must specify the name of the secret! In this example, the secret\u0026rsquo;s name is smdemo. The subsequent commands select the database, display the table names in the database, query the rows in the table, and exit MySQL.  ./mysql.newway.sh smdemo use smdemo; show tables; select * from bookinfo; quit; You can see an example of the output below. This shows that you can access the database, the \u0026ldquo;new\u0026rdquo; way, using AWS Secrets Manager. Rotate the secret In this section, you will enable the rotation of the secret you created in AWS Secrets Manager.\n Go to the Secrets Manager console. Check your region to make sure the Secrets Manager console is operating in the ap-southeast2 region. Click on the secret that you previously created. Click Edit rotation. Select Enable automatic rotation. Choose 30 days for the rotation interval. Select the Create a new Lambda function to perform rotation radio button. This will cause Secrets Manager to use CloudFormation on your behalf to create a rotation function using the AWS Serverless Application Repository. If you had customized your own rotation function or if you were using a credential for a special application, you would select that here. Enter a name for the rotation function. In the figure below, we used smdemo but you can select whatever you wish. You now need to select the secret whose permissions will be used to rotate the secret. For this Builder Session, select Use this secret. This will tell the rotation function to access the RDS database using the secret and rotate the same secret.  If your application supports two classes of users, for example a \u0026ldquo;superuser\u0026rdquo; and a \u0026ldquo;normal privilege\u0026rdquo; user, you could select Use a secret that I have previously stored in Secrets Manager and use the \u0026ldquo;superuser\u0026rdquo; credential to rotate the credential for the \u0026ldquo;normal privilege\u0026rdquo; user.\nYour window should look similar to the figure below.  Click Save. You will see a message telling you that the rotation is beginning and that you should remain on the page until it is complete. AWS Secrets Manager is now using the AWS Serverless Application Repository to install an AWS Lambda rotation function on your behalf. Do not leave this page until the rotation is complete.   If you see the following error message, goto the AWS Lambda and verify the Lambda function secretsManagersmdemo exists, than follow the above Rotate steps again but this time selecting Use an existing lambda function to perform rotation and choose secretsManagersmdemo Lambda function to enable automatic rotation.  A message will appear when the rotation is complete. Refresh your browser window to update your any cached fields.  Click Retrieve secret value to see the new password value.  Access the database Let\u0026rsquo;s try to connect to the database again, both the \u0026ldquo;old\u0026rdquo; way with a hard-coded password, the \u0026ldquo;new\u0026rdquo; way with AWS Secrets Manager.\n On the bastion host, run the command below to access the database using the hard-coded password.  ./mysql.oldway.sh You should receive an error message (access denied) because the mysql.oldway.sh script has the same hard-coded password.  Run the commands below to access the database with Secrets Manager by running the commands below. Note that you must specify the name of the secret! In this example, the secret\u0026rsquo;s name is smdemo. You should be able to access the database as you did before because Secrets Manager fetches the new credential rather than using a hard-coded credential.  ./mysql.newway.sh smdemo use smdemo; show tables; select * from bookinfo; quit; You have completed ths RDS phase and have learned how to use AWS Secrets Manager with Amazon RDS. You will now continue to the Fargate phase.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module3/host-assess/",
	"title": "5. Host Layer - Assess",
	"tags": [],
	"description": "",
	"content": "Identifying and Remediating Host Vulnerabilities - Host Layer Round In the previous phase, CloudFormation deployed a stack containing some Amazon EC2 instances behind an application load balancer. You are now going to learn about AWS Inspector. AWS Inspector assesses the instances and identifies security findings that you will later remediate.\nExplore Amazon Inspector You are now going to learn about AWS Inspector terms and explore the Inspector console.\nUnderstanding Amazon Inspector targets   Go to the Amazon Inspector console.\n  Click Assessment Targets on the left menu. Assessment targets represent a group of EC2 instances that Inspector will assess. You will see a target whose name begins with InspectorTarget. Click on the arrow widget to open the target and display the details. You should see something similar to the image below.\nYour stack name will be something like \u0026quot;mod-\u0026quot; and be followed by some additional characters. This means that this Inspector target is configured to select all instances that are related to the CloudFormation stack with the given stack name.\nTags are labels. They are very helpful when you are working with variable numbers of instances. Rather than specifying individual instance ids, you can select the instances with a specific tag and then do operations on that group. In the case of instances behind a load balancer, you may not know the instance IDs, but if they all share a tag then you can work on them collectively by tag.\n  Click the Preview Target button. A new window opens as shown below.\nYou now see there are three Instances that Inspector will assess based on the configuration of the target.\n  Click the Assessment Templates on the left menu. An assessment template appears as shown below.\nYou will see an assement template whose name begins with InspectorTemplate. Assessment templates represent the selection of a target and one or more rules packages. A rules package is a collection of rules that represent security checks. This template assesses the previously mentioned target against the following two rules packages:\nCommon Vulnerabilities and Exposures: The rules in this package help verify whether the EC2 instances in your assessment targets are exposed to common vulnerabilities and exposures (CVEs). Attacks can exploit unpatched vulnerabilities to compromise the confidentiality, integrity, or availability of your service or data. The CVE system provides a reference method for publicly known information security vulnerabilities and exposures. For more information, see https://cve.mitre.org. You typically remediate findings from this rules package by installing patches.\nSecurity Best Practices: The rules in this package help determine whether your systems are configured securely. For example, one rule in this package checks to see if root login has been disabled over ssh. You typically remediate the findings by adjusting configuration settings.\n  On the Amazon Inspector menu, click Assessment runs. You should see an entry for an assesment that was started on your behalf. CloudFormation ran this for you to save time. If the status is not, Analysis complete, then periodically refresh the screen until the status changes to Analysis complete as shown in the figure below.\n  On the line that represents your most recent run, make note of the number in the Findings column (117 in this diagram). After you perform the remediation later in this workshop, that number should decrease. Click on the number in the Findings column. The findings associated with the run will appear as shown below.\nYou will see one of the findings has been expanded to reveal more details. The middle section of the finding has been removed to save space.\n  Now that you have learned about Inspector assessments, you are ready to perform some remediation using AWS Systems Manager Patch Manager. You will then run an Inspector assessment yourself to see if the number of findings has changed.\nClick here to proceed to the Remediate Phase.\n   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module3/host-remediate/",
	"title": "6. Host Layer - Remediate",
	"tags": [],
	"description": "",
	"content": "Identifying and Remediating Host Vulnerabilities - Host Layer Round In the previous Assess Phase, you installed Amazon Inspector on the instances that were launched as a result of the CloudFormation stack. You will now use AWS Systems Manager Patch Manager to apply patches. You will use tags to select the instances as well.\nIn this section you will do the following tasks:\n Use AWS Systems Manager Patch Manager to set up patching Use AWS Systems Manager Run Command to check the status of the patching  Use AWS Systems Manager Patch Manager   Go to the Systems Manager console and select Patch Manager from the menu on the left. If you see the Patch Manager home screen, then click the View predefined patch baselines link as shown below:\n  You will now see a list of default patch baselines that are used to patch each operating system supported by Patch Manager. The default patch baseline only patches major security issues. You are going to create a new Amazon Linux 2 patch baseline that will patch more things and make this new patch baseline the default. Click Create patch baseline.\n  In the Name field, enter a name to give the new baseline such as pww. In the Operating system field, select Amazon Linux 2. Select All for the Product, Classification, and Severity dropdown menus. In the Approval rules section, check the box under Include non-security updates. IMPORTANT NOTE: Depending on the size of the screen, the box may not align with the title Include non-security updates. See the figure below.\n  Click the Create patch baseline button at the bottom of the screen. You should now see the new patch baseline in the list of baselines. You may need to refresh the browser window to see it. This new patch baseline includes non-security patches. Note at the end of the line representing the newly created patch baseline you will see No in the Default Baseline column as shown in the figure below.\n  Click the radio button on the line with the newly created patch baseline. From the Actions menu at the top select Set default patch baseline. You will be asked to confirm this. You have just set the default patch baseline for Amazon Linux 2 to use the patch baseline you just created that includes non-security patches. You should now see Yes at the end of the patch baseline as shown below.\n  Click Configure patching. In the Configure patching screen, go to the Instances to patch section and click the Enter instance tags radio button. In the Instance tags field, enter aws:cloudformation:stack-name into the Tag key field. In the Tag value field, enter the Cloudformation stack name - the stack name in Cloudformation would start with mod- with the description WAF Workshop Demo and be followed by some numbers. Click Add.\n  In the Patching schedule section, click the Skip scheduling and patch instances now radio button.\n  In the Patching operation section, click the Scan and install radio button if it is not already selected. Your screen should look similar to the image below.\n  Click the Configure patching button at the bottom of the window. You will see a message at the top of your screen saying that Patch Manager will use Run Command to patch the instances. Run Command is another feature of AWS Systems Manager that runs a command across multiple Amazon EC2 instances. Patch Manager builds the commands necessary to perform the patching and is using Run Command to actually execute the commands.\n  Check the patching status You are now going to examine the status of the patching operation by using AWS Systems Manager Run Command.\n  Go to the AWS Systems Manager Console and click Run Command on the left menu. If the patching is still running, you will see the entry in the Commands tab. Wait for the command to finish. Refresh the screen if necessary to update the display. Once the command has finished, click on Command history.\n  Look for the line containing the document name AWS-RunPatchBaseline. That represents the Patch Manager activity.Your screen should look similar to the image below. If the command status is Failed, click on the Command ID link, click Rerun to reapply the patch baseline and monitor the status.\n  Click on the Command ID link to see more details about the command. You will see a line for each target with a link referencing the Instance ID. If you then click on the Instance ID, you will see each step of the command that is executed. Note that some steps are skipped because they do not apply to the operating system of the instance. Also, you only see the first part of the command output. If you want to see all of the output you can configure Systems Manager to direct the output into an Amazon S3 bucket.\nYou have now completed the patching operation. In the Verify Phase, you will re-assess the environment with Amazon Inspector.\n  Click here to proceed to the Verify Phase.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module4/fargate/",
	"title": "3. Fargate with Secrets Manager",
	"tags": [],
	"description": "",
	"content": "Prerequisites  You must complete the RDS phase of this builder session before starting this phase. You should be familiar with Docker concepts.  Overview In this phase, you will learn how to use AWS Secrets Manager with AWS Fargate. AWS Fargate is a compute engine for Amazon Elastic Container Service that allows you to run containers without having to manage servers or clusters. With AWS Fargate, you no longer have to provision, configure, and scale clusters of virtual machines to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing. AWS Fargate removes the need for you to interact with or think about servers or clusters. Fargate lets you focus on designing and building your applications instead of managing the infrastructure that runs them.\nThe CloudFormation template for this workshop round created a Dockerfile and some helper shell scripts. You will use these shell scripts to build the Docker image and push it to Amazon Elastic Container Registry (ECR), a fully-managed Docker container registry. This guide does not go into the details of these scripts. You are encouraged to read them to learn what they do to gain a better understanding of Docker, Amazon ECR, and AWS Fargate.\nLastly as a reminder, the environment provisioned by CloudFormation is shown in the figure below\nBuild and push the Docker image  If you are do not have a session open to the bastion host, then connect to the bastion host using AWS Systems Manager Session Manager. To do this:  Go to the Systems Manager console. Click Session Manager. Click Start session. Select the radio button for the instance called smdemo-host. Click Start session.   The scripts you will be using are owned by the ec2-user account. If you are not currently using ec2-user as your effective user id, then enter the command below to change your effective user id and directory to those of ec2-user: sudo su - ec2-user  Run this script to build the Docker image: ./dockerbuild.sh This command creates the Docker image based on the Dockerfile that was generated by CloudFormation. The build process can take a few minutes to complete. Wait until the shell prompts you for another command before continuing.\n Confirm that the image was built by using this command: docker images You should see a repository whose name contans -ecrre- that was recently built.\n Run the following script to push the image to Amazon ECR to make the image available to AWS Fargate: ./dockertagandpush.sh This script will take a few minutes to complete. Wait until the shell prompt appears before continuing.\n  You have now built the Docker image and pushed it to Amazon ECR. You will now configure AWS Fargate.\nConfigure and launch AWS Fargate You will now configure the Amazon Elastic Container Service Task Definition that AWS Fargate will use to launch the task. After you confgure the task definition, you will launch the AWS Fargate task.\n Go to the ECS console and select the Clusters menu item. You should see a cluster whose name begins with the stack name you chose and contains the string -ECSCluster-. The cluster description should look similar to that shown in the figure below.  Click the Task Definitions menu item. You should see a task definition whose name contains -TaskDefinition- as shown below.  Click the check box next to the appropriate task definition name and then click Create new revision. Leave all of the current values in place. Scroll down and click Configure via JSON. Look for the list named secrets. It should look similar to what is shown in the figure below. Replace SECRETNAME with the name of the secret you stored in the RDS phase which should be smdemo. Click Save to save the revised JSON definition. Click Create to create the new revision of the Task Definition that includes the JSON revisions. You will see a message saying that the new revision has been created. Notice that the revision has a version number attached to it as shown in the figure below.  Click the Actions button and select Run Task from the dropdown menu. Choose the values listed in the table below.     Field Value     Launch type Fargate   Cluster VPC The VPC that was created by CloudFormation with the 10.200.0.0/16 CIDR   Subnets Select all of the subnets listed   Security group Select the group whose name includes BastionSG    Leave all other values at their current settings.\n Click Run Task . Refresh the Task window at the bottom to update the status. The status will start at PROVISIONING, then change to PENDING, and then to RUNNING*.  Now that you have launched the Fargate task, you are ready to connect to the container.\nConnect to the AWS Fargate container  Click on the task name to display details of the task and note the private IP associated with the task. The private IP should begin with 10.200. The third octet should be either 11 or 12. This private IP of the Fargate task has been connected to your VPC. From the Systems Manager Session Manager session you started above, while sudo\u0026rsquo;d as ec2-user, enter the following command: ssh PrvateIpOfFargateTask Replace PrivateIPOfFargateTask with the private IP of the Fargate task. You will be prompted for the password. Enter the value from the EC2UserPassword output value from the CloudFormation stack. You should now see a shell prompt.\n  Access the database Now that you have connected to the AWS Fargate container, you can now access the RDS database. After you access the database you will learn how the scripts receive the secret value from the ECS Task Definition.\n Run the command below to access the RDS database. Note that you do not supply the name of a secret since that has been passed in the task definition. ./mysql.newway.sh use smdemo; show tables; select * from bookinfo; quit; The output should be similar to what you see in the figure below.   Examine the flow of the secret Now that you have been able to access the database through Fargate, it\u0026rsquo;s important to understand how the value made its way from the Task Definition to the mysql.newway.sh script. It\u0026rsquo;s important to understand that there may be multiple ways to accomplish this depending on what is running in the container. Since this container is running a Linux shell, the path the secret value flowed took advantage of the capabilities of the shell.\n  In the task definition JSON that you modified above, you modified the value of the valueFrom key to be the ARN of the secret that you stored in Secrets Manager. The corresponding name key has as its value the name of an environment variable that will be presented to the Fargate task container when it is instantiated.\n  The Dockerfile that is used to build the Docker image for Fargate has the following as its last line:\nCMD /home/ec2-user/startprocesses.sh\nThis means that when Fargate launches the Docker image, the image will run the startproceses.sh file.\nHere are the contents of the main portion of that file:\ntouch /etc/profile.d/ecs.sh chmod 644 /etc/profile.d/ecs.sh env | \\ grep \u0026quot;^TASKDEF_\u0026quot; | \\ awk -F= '{printf \u0026quot;export %s=%c%s%c\\n\u0026quot;, $1, 39, $2, 39 }' \\ \u0026amp;gt;\u0026amp;gt; /etc/profile.d/ecs.sh You can see that the startprocesses.sh script creates another script named /etc/profile.d/ecs.sh. Each line in ecs.sh contains the definitions of every environment variable passed to the container whose environment variable names start with TASKDEF_. Since the file exists in the /etc/profile.d directory, each time a user logs in, the ecs.sh file willl be \u0026ldquo;sourced\u0026rdquo; by the shell causing the environment variables to be set for the logged in user.\n  To confirm that the secret value has been passed, enter the following command from the Fargate container shell prompt:\nenv|grep TASKDEF_SECRET You should see a value similar to that in the figure below. This shows that the Task Definition properly retrieved the secret information and passed it to the Fargate container which in turn made it available to the login shell.\n  Summary You have completed ths Fargate phase and have learned how to use AWS Secrets Manager with AWS Fargate.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module3/host-verify/",
	"title": "7. Host Layer - Verify",
	"tags": [],
	"description": "",
	"content": "Identifying and Remediating Host Vulnerabilities - Host Layer Round - Verify Phase Now that you have remediated the environment, you will again use Amazon Inspector to assess the environment again to see how the patching affected the overall security posture of the environment. You will first run an Inspector assessment. While the assessment is running, you will explore some other AWS capabilities. Lastly, you will return to Inspector to see the results of the new assessment to the effects of the patching that you did with Systems Manager Patch Manager.\nRun another Inspector assessment   Go to the Amazon Inspector console, click Assessment templates on the menu.\n  Locate the template that you created during the Assess Phase and check the box at the left end of that row.\n  Click Run. This will launch another assessment run.\n  The assessment run will take 15 minutes to complete.\n  Explore AWS Systems Manager Maintenance Windows While Inspector is running, you will learn about AWS Systems Manager Maintenance Windows. AWS Systems Manager Maintenance Windows let you define a schedule for things like patching an operating system. So, rather than applying patches once as you did earlier, you can set up a maintenance window to apply patches on an ongoing basis. Each maintenance window has a schedule (when the maintenace is to occur), a set of registered targets for the maintance (in this case, the Amazon EC2 instances that are part of this workshop), and a set of registered tasks (in this case, the patching operation).\nCreate the maintenance window   Go to the Systems Manager console window and select Maintenance Windows.\n  Click Create maintance window. Use the values in the table below. You can leave all other defaults in place. Note the future start date of the window. This is done to avoid interfering with the Inspector scan that is currently running.\n     Field name Field Value     Name pww_mw   Specify with Cron schedule builder   Window starts Every 12 hours   Duration 1 hour   Stop initiating tasks 0 hours   Window start date 12/01/2099   Schedule time zone Select your timezone    Your screen should look similar to the figure below. Click Create maintenance window to save the maintenance window.  Register the Maintenance Window target   In the list of maintenance windows, click on the id of the window corresponding to the maintance window you just created. The link begins with a mw- prefix.\n  Click the Actions button and select the Register targets menu item. Use the values in the table below. Leave all other fields at their default values.\n     Field name Field Value     Target name pww_targets   Specify instance tags Select the radio button   Tag key aws:cloudformation:stack-name   Tag value the stack name (should start with \u0026ldquo;mod-\u0026rdquo; with the description of WAF Workshop Demo in the CloudFormation console)    Make sure you click Add to add the tag. Your screen should look similar to the figure below.\n Click Register target to register the target to the maintenance window based on the information you entered.  Register the Maintenance Window task   Click Maintenance windows on the left menu.\n  In the list of maintenance windows, click on the id of the window corresponding to the maintance window you just created. The link begins with a mw- prefix.\n  Click the Actions button and select the Register Run command task menu item. Use the values in the table below. Leave all other fields at their defualt values.\n     Field name Field Value     Task name pww_task   Command document AWS-RunPatchBaseline   Target by Selecting registered target groups   Concurrency 1 targets   Error threshold 1 errors   Service role option Use the service-linked role for Systems Manager   Operation Install    Your screen should look similar to the figure below.\nClick Register Run command task to register the patching task to the maintenance window based on the information you entered.  You have now completed the definition of a Systems Manager Maintenance Window. The purpose of this additional task is to show you how you could implement patching on an ongoing basis in your environment.\nExamine the results of the Inspector assessment Now that you have explored some additional AWS capabilities, you will examine the results of the second Inspector assessment.\n  Go to the Inspector console. Click Assessment runs and periodically refresh the screen. Wait until the status for the run changes to Analysis complete. The run will take approximately 15 minutes to complete.\n  Compare the number of findings between the two runs. In most cases, there will be fewer findings in the newer run since patches have been applied. The change in the number findings may vary based on the age of the AMI used to launch the instances.\n  Click the number of findings for the newest run (after the patches were installed). You will then see all of the findings that were not patched during the Remediate Phase.\n  Take a look at the entries that were not patched. A common example of a finding is an instance is configured to allow users to log in with root credentials over SSH, without having to use a command authenticated by a public key. Why would Patch Manager not patch this or the other findings?\n  Congratulations! You have now completed this workshop!\n   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module1/evaluate/",
	"title": "2. Evaluate Session Manager Configuration",
	"tags": [],
	"description": "",
	"content": "To begin our evaluation, we need to examine the default configuration and behavior of Session Manager and determine what is needed to meet the requirements given to us for this project.\nEvaluate cross-platform behavior, security context and default privilege levels\n Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, browse to the Managed Instances console. Select the session-manager-linux-prod instance, click Actions, click Start Session.  Now that we have established an interactive shell to the instance, let\u0026rsquo;s determine our user context on the instance and evaluate our privilege level. Click into the Session Manager Console to be able to enter commands below:  Execute whoami to determine what user context we\u0026rsquo;re running under. Execute sudo su followed by cat /etc/sudoers.d/ssm-agent-users to evaluate the privilege level assigned to the user ssm-user.    AWS Systems Manager works by having the SSM agent on the instance establish an outbound session to the Systems Manager service.  Execute netstat -nputw | grep -i ssm to validate this behavior and verify communications are actually being sent over a secure channel.    Click Terminate to terminate the session or type exit and select close . Select the session-manager-windows-prod instance, click Actions, click Start Session.  Since this is a Windows instance, notice we\u0026rsquo;ve established an interactive PowerShell session.  Execute whoami to determine what user context we\u0026rsquo;re running under. Execute net user ssm-user to determine the local group memberships assigned to the local user ssm-user.    Click Terminate to terminate the session or type exit and select close.   Evaluate the default auditing and logging configuration for Session Manager\n Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, browse to the Session Manager console. Select the Session history tab and review the session history details available. Note: that no information is presented under the Output location field.   Evaluate the port requirements and default permissions of managed IAM policies\n Browse to the EC2 Instances console Click on any instance with its name starting with \u0026ldquo;session-manager\u0026rdquo;.  In the pane to the bottom right under the Description pane, click on the session-manager-demo Security Group.  Select the Inbound rules and Outbounds rules tab to evaluate the Security Group rules. You should notice that no inbound ports have been authorized and all outbound traffic is authorized.   Lets now review the permissions granted by the managed IAM policy. Browse to the IAM console and under Policies click on AmazonSSMManagedInstanceCore. Note the permissions granted as shown in the image below.\nThis managed policy is attached to the session-manager-demo-default IAM role currently associated with our managed instances which can be seen under the Policy usage tab.\n  Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module0/",
	"title": "Lab Setup",
	"tags": [],
	"description": "",
	"content": " If you are using your own AWS account then you can skip this page and go straight into the labs.\n Open AWS Event Engine Enter the 12-digit hash (provided through GotoWebinar) and click proceed. Once successfully logged in, click on AWS Console to log into the console "
},
{
	"uri": "/workshops/module2/sysmgrlabsetup/",
	"title": "1. AWS Systems Manager Lab Setup",
	"tags": [],
	"description": "",
	"content": "Lab Setup In this part of the lab you will set up the necessary resources in your environment to proceed with the Systems Manager labs.\nCreate an EC2 Key Pair   Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\nIn 2019 the EC2 console was updated to have a consistent look and feel with other AWS Services, and to improve the user experience. This lab guide shows the new and improved console experience. You can enable and disable this new look based on your preferences.\n   In the left-hand menu look for the Network \u0026amp; Security section, and click on Key Pairs. Now click on the Create Key Pair button at the top right of the console.  In the Create Key Pair dialog box, type a Key pair name such as SSMLabKeys, leave the file format setting on the default of pem and then click on Create key pair.  Save the SSMLabKeys.pem file to your local machine. Note: You will not need the keys for this lab, they are only required for the CloudFormation Stack to deploy successfully (below, in the section: To deploy the lab infrastructure).  KEY CONCEPT: Management Tools - AWS Systems Manager  AWS Systems Manager is a collection of features that enable IT Operations, some of which we will explore throughout this lab.\nThere are set up tasks and prerequisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments.\n Verify that your instances run a supported operating system. For EC2 instances, create an IAM instance profile and attach it to your machines. For on-premises servers and VMs, create an IAM service role for a hybrid environment. Verify that you are allowing HTTPS (port 443) outbound traffic to the Systems Manager endpoints. (Recommended) Create a VPC endpoint in Amazon Virtual Private Cloud to use with Systems Manager. On on-premises servers, VMs, and EC2 instances created from AMIs that are not supplied by AWS, install a Transport Layer Security (TLS) certificate. For on-premises servers and VMs, register the machines with Systems Manager through the managed instance activation process. Install or verify installation of SSM Agent on each of your managed instances.  SSM Agent is installed by default on (base Amazon-managed AMIs):\n Windows Server 2008-2012 R2 AMIs published in November 2016 or later Windows Server 2016 and 2019 Amazon Linux Amazon Linux 2 Ubuntu Server 16.04 Ubuntu Server 18.04 Amazon ECS-Optimized  Setting up Systems Manager   Use your account to access the Systems Manager console. Choose Managed Instances from the navigation menu on the left. If you have satisfied the prerequisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page.  As a user with AdministratorAccess permissions, you already have User Access to Systems Manager. The Amazon Linux AMIs used to create the instances for this lab are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default.    AWS Systems Manager now offers a Quick Setup method to simplify configuring your instances to be managed.\n  Click on the Quick Setup link at the top of the navigation menu on the left.\n  Review the Permissions (Required) section. Note that there is a default role, and you can also specify your own role for more granular permissions.\n  Keep the selections Instance Profile role and Assume role for Systems Manager on the Use the default role selections.\n  Review the Quick Setup options section\n  Ensure that the Update Systems Manager (SSM) Agent every two weeks option is checked. Leave all other options unchecked.\n    In the Targets section, define targets for systems manager setup:\n Under Target selection method, select Choose all instances in the current AWS account and region    Select the Enable button.\n  To deploy the lab infrastructure:  Deploy the infrastructure with CloudFormation - Click here To Deploy Lab into your Account  On the Create Stack page, leave all settings on their defaults and click on Next. In the Specify stack details section, accept the predefined stack name SSMPatchLabStack1. In the Parameters section:  Specify the InstanceProfile as AmazonSSMRoleForInstancesQuickSetup. MAKE SURE TO ENTER THIS EXACTLY AS WRITTEN OR THE STACK WILL NOT DEPLOY! Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list (SSMLabKeys).  Enter the loop-back IP address 127.0.0.1/32 in SourceLocation.   Define the Workload Name as Prod. Click on Next.    On the Configure stack options page under Tags, type Owner in the Key field, and enter your name in the Value field. Leave all other sections unmodified. Scroll to the bottom of the page and click on Next.  On the Review page, review your choices and then scroll down and click on Create stack.  On the CloudFormation console page your stack deployment will now begin. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. Deployment will take about five minutes.   Check Deployed Instances When the Status of your stack displays CREATE_COMPLETE in the filter list, you will have created a representation of a typical \u0026lsquo;lift and shift\u0026rsquo; 2-tier application migrated to the cloud.  Navigate to the EC2 console to view the deployed systems. Click on Instances in the left-hand menu. The four instances created for this lab will be prefixed with \u0026lsquo;Prod-\u0026rsquo; in the Name column.   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module1/configure/",
	"title": "3. Configure Systems Manager Session Manager",
	"tags": [],
	"description": "",
	"content": "As we observed during our initial evaluation, our activity within a session is not yet being logged. In this step, we are going to configure Session Manager to store session log data in a specified Amazon S3 bucket for auditing purposes. The default option is for logs to be sent to an encrypted S3 bucket. Encryption is performed using the key specified for the bucket.\nCreate a Custom Policy for Amazon S3 Bucket Access\nCreating a custom policy for Amazon S3 access is required only if you are using a VPC endpoint or using an S3 bucket of your own in your Systems Manager operations.\n  Browse to the CloudFormation console, and select the stack starting with mod- and Description Session Manager workshop. Select the output tab and note down the S3 bucket name with the key value of Logging Bucket. This bucket will follow the naming convention session-manager-demo-[ACCOUNT_NUMBER]-ap-southeast-2 as below.   Open the IAM console\n  In the navigation pane, choose Policies, and then choose Create policy.   Choose the JSON tab, and replace the default text with the following:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:PutObjectAcl\u0026quot;, \u0026quot;s3:GetEncryptionConfiguration\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::my-bucket-name/*\u0026quot;, \u0026quot;arn:aws:s3:::my-bucket-name\u0026quot; ] } ] }  Replace my-bucket-name with the bucket name from step 1.\n  Choose Review policy.   For Name, enter a name to identify this policy, such as SSMInstanceProfileS3Policy or another name that you prefer.   Choose Create policy.\n  Attaching Policy to Instance Profile Role\n After the policy has been created, open the IAM console and head into Roles in the navigation menu to the left and search for session-manager-demo-default.  Then, click on Attach policies. In the Filter, enter SSMInstanceProfileS3Policy as the policy to select. Select the policy and click on Attach policy.  Enable Session Logging for Session Manager\nOnce the policy has been created and associated with our Instance Profile, we will configure session logging to the S3 bucket we created using CloudFormation.\n  Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, browse to Session Manager and select Preferences tab and Edit option\n  Under the Write session output to an Amazon S3 bucket heading, select S3 bucket.\n  Leave Encrypt log data selected. In the S3 bucket name field, select the bucket you noted down at the start of this step. In the S3 key prefix field, enter Session-Manager. Click Save   Select the Sessions tab, click Start Session, select a Linux instance and click Start session.   Execute some basic commands to demonstrate session logging is working as expected:\n ps -ef | grep -i ssm sudo ls -l ../etc whoami exit terminate the session.    Browse to the Session History tab and locate our last session. Wait for session Status to change from Terminating to Terminated. In the Output location column, click Amazon S3 to view the session log.   Observe the data captured in the session log includes all input and output of the commands we entered.\n  Note: The S3 bucket created for you is configured to use S3 Default Encryption with AWS S3-managed keys (SSE-S3). All session log data will be encrypted by default but you can also choose to use your own KMS Customer Master Key (SSE-KMS).\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module2/systemsmanager/",
	"title": "2. AWS Systems Manager: Operations as Code",
	"tags": [],
	"description": "",
	"content": "Operations as Code with Systems Manager  In a traditional environment you would have to set up the systems and software to perform administration activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems.\nOperations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \u0026ldquo;swivel chair\u0026rdquo; processes that require multiple interfaces and systems to complete a single operations activity.\nSetting up Systems Manager to capture tagged targets   Open the Systems Manager console. In the Systems Manager console select Quick Setup from the left-hand navigation menu. Click on the Edit all at the top right corner of the console. Scroll down and change Targets to Specify instance tags.  Under Tags specify Environment for the key and SSMPatchLab for the value.    You can constrain managed instances to those with specific tags, such as Environment or Workload.\nYou can also manually select specific instances for management, though not recommended for your environment as it scales.\n Select the Reset button at the bottom of the page.  KEY CONCEPT: What did we learn?  The requirements and basic functionality of AWS Systems Manager and the SSM Agent How to configure Systems Manager and your instances so that Systems Manager can manage them   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module2/inventory/",
	"title": "3. AWS Systems Manager: Inventory",
	"tags": [],
	"description": "",
	"content": "Inventory with Systems Manager   If you are using an AWS Event Engine account you might see an error at the top of your screen when accessing features with resource group integration. These errors can safely be ignored, they will not impact your ability to perform any of the steps in the lab.\n  KEY CONCEPT: AWS Systems Manager - Inventory AWS Systems Manager collects information about your instances and the software installed on them, helping you to understand your system configurations and installed applications.\nYou can collect data about applications, files, network configurations, Windows services, registries, server roles, updates, and any other system properties.\nThe gathered data enables you to manage application assets, track licenses, monitor file integrity, discover applications not installed by a traditional installer, and more.\nUsing Systems Manager Inventory to Track Your Instances   Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, choose Inventory.  Scroll down in the window to the Corresponding managed instances section. Inventory displays the instance data provided by the EC2 service. Click on the InstanceID of one of your systems that has the prefix Prod- in the Name column. Examine each of the available tabs of data under the Instance ID heading (Description/Inventory/Tags/Associations/Patch/Configuration Compliance). Note that Inventory is currently empty, as it is unconfigured.    Inventory collection must now be specifically configured and the data types to be collected need to be specified:  Choose Inventory from the left-hand menu. Click on the Setup Inventory button at the top right corner of the window.    In the Setup Inventory screen, define targets for inventory:  Under Specify targets by, select Specifying a tag Under Tags specify Environment for the key and SSMPatchLab for the value     You could select all managed instances in an account, ensuring that all managed instances would be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. You can also manually select specific instances for inventory, although this is not recommended for your environment as it scales.   You can Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes. Under Parameters you can filter what information to collect with the inventory process - review the options and keep the defaults. Click on the Setup Inventory button at the bottom right of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance).  You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager.\n Congratulations! You have now created an inventory specification and associated it with your lab instances.\n What did we learn?  The methods to setup AWS Systems Manager to collect inventory from your instances The options for scheduling inventory collection on your instances How to check compliance for inventory collection across your environment   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module1/portforward/",
	"title": "4. Use Port Forwarding For Web Redirection",
	"tags": [],
	"description": "",
	"content": "Session Manager Port Forwarding feature allows you to tunnel data from remote port on instance to a local port on client machine. This enables web redirection for user without opening inbound ports. You can use this feature using AWS CLI which requires you to install session-manager-plugin on client machine. It uses public SSM document AWS-StartPortForwardingSession that allows users to provide local and remote port numbers to enable port forwarding.\nReview SSM Document\n Under Shared Resources in the AWS Systems Manager navigation menu, browse to the Documents console and review contents of AWS-StartPortForwardingSession document. Note that Session Type is Port and default value for portNumber is 80.  Install Apache HTTP Server on EC2 instance\n  Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, browse to the Session Manager console and start a session to linux instance session-manager-linux-stage.\n  Type command below\nsudo yum -y install httpd; sudo systemctl enable httpd; sudo systemctl start httpd  You should see an output as shown below.   Verify apache http server(httpd) is running on port 80 by running command\nsudo netstat -atnp | grep -i httpd  You should see an output as shown below.   Start Port Redirection\n Browse to the EC2 Console and note instance-id for instance session-manager-linux-stage.   Browse to the AWS Cloud9 IDE and type below command in the console after replacing with appropriate instance ID to start a session to session-manager-linux-stage instance.\naws ssm start-session --target \u0026lt;instance-id\u0026gt; --document-name AWS-StartPortForwardingSession --parameters \u0026quot;localPortNumber=8080\u0026quot;    You should see a message indicating port 8080 has been opened for this session.\n   Within Cloud9 to preview a web page, select Preview from the menu option and Preview Running Application as shown below. You should be able to access Apache http server home page which is running on port 443 on remote instance session-manager-linux-stage.   Press Control+C on terminal to terminate the session.   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module2/statemgr/",
	"title": "4. AWS Systems Manager: State Manager",
	"tags": [],
	"description": "",
	"content": "State Manager with Systems Manager  KEY CONCEPT: AWS Systems Manager - State Manager In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state.\nAn association defines the state you want to apply to a set of targets. An association includes three mandatory components and one optional set of components:\n A document that defines the state Target(s) A schedule (Optional) Runtime parameters.  When you performed the Setup Inventory actions in the previous step, you created an association in State Manager.\nKEY CONCEPT: AWS Systems Manager - Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren’t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.\nReview Association Status   Under Instances \u0026amp; Nodes in the navigation menu on the left, select State Manager (At this point, the Status may show that the inventory activity has not yet completed - if not, refresh the screen until all associations have a status of success).  Click on the blue Association id that is the result of your Setup Inventory action (the Association Name for the correct association id you want to view for this lab is Inventory-Association).  Examine each of the available tabs of data under the Association ID heading.  Click on the Edit button at the top right of the console. Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name). Scroll to the bottom of the screen and click on Save Changes.    Inventory is accomplished through the following:\n The activities defined in the AWS-GatherSoftwareInventory command document. The parameters provided in the Parameters section are passed to the document at execution. The targets are defined in the Targets section. In this example there is a single target, the wild-card. The wild-card matches all instances making them all targets.\n  The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. There is the option to specify Output options. Note: If you change the command document, the Parameters section will change to be appropriate to the new command document.  Navigate to Managed Instances under Instances and Nodes in the navigation menu on the left. An Association Status has been established for the inventoried instances under management. Click on one of the blue Instance ID links (for one of the instances that has a \u0026lsquo;Prod-\u0026rsquo; name prefix) to go to the details of the instance. The Inventory tab is now populated; you can track associations and their last activity under the Associations tab.  Navigate to Compliance under Instances \u0026amp; Nodes in the navigation menu on the left. Here you can view the overall compliance status of your managed instances in the Compliance resources summary  Navigate to Inventory under Instances \u0026amp; Nodes in the navigation menu on the left, to view individual compliance status of systems in the Corresponding managed instances section.   The inventory collection activity can take up to 10 minutes to complete. Whilst waiting for the inventory activity to complete, you can continue with the next section.\n KEY CONCEPT: What did we learn?  The functionality of the Systems Manager State Manager feature The types of associations that State Manager can create How to setup State Manager associations for your environment How to view and edit State Manager associations   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module1/ssh/",
	"title": "5. Enable SSH Through Session Manager",
	"tags": [],
	"description": "",
	"content": "Session Manager can be configured to connect to remote instance using Secure Shell(SSH) without opening inbound port or maintaining bastion host. You can also copy files between local and remote machine using Secure Copy Protocol(SCP). This feature uses public SSM document AWS-StartSSHSession.\nReview SSM Document\n Browse to the Systems Manager Document console and review content of AWS-StartSSHSession document. Note that Session Type is Port and Default Port provided is 22.  Launch an EC2 Instance\n Browse to the EC2 Console and click Launch Instance. Choose below configurations for the instance.  AMI: Amazon Linux 2 Instance Type: t2.micro Network: smdemo-vpc IAM role: session-manager-demo-default Storage: leave default value Tag: Key -\u0026gt; Name, Value -\u0026gt; session-manager-demo-linux-ssh Security Group: Remove SSH rule from the launch wizard Security Group Key Pair: Select MyKeyPair    Configure ssh proxy command\n  Browse to the AWS Cloud9 IDE and type the following command\nnano ~/.ssh/config    Add below proxy command and save the file (to save, type CTRL-X, type Y and hit Enter).\n host i-* mi-* ProxyCommand sh -c \u0026quot;aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'\u0026quot;    Run the following command to provide adequate permissions to the file.\n chmod 600 ~/.ssh/config    SSH to EC2 instance\n Browse to the EC2 Console and note instance-id for instance session-manager-demo-linux-ssh. Type command ssh -i MyKeyPair.pem ec2-user@[INSTANCE-ID] to ssh to the instance session-manager-demo-linux-ssh using PEM file generated during instance launch. Type yes and press enter to confirm. Congratulations! You should be successfully logged in to the instance and see output as below.  Type exit on terminal to terminate the session.  Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module2/patch/",
	"title": "5. AWS Systems Manager: Patch Manager",
	"tags": [],
	"description": "",
	"content": "Patch Management with AWS Systems Manager  KEY CONCEPT: Systems Manager - Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates.\nFor Linux-based instances, you can also install patches for non-security updates.\n You can patch fleets of Amazon EC2 instances and/or your on-premises servers and virtual machines (VMs) by Operating System type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), Oracle Linux, CentOS, Debian, Raspbian, Amazon Linux and Amazon Linux 2. You can scan instances to see a report of missing patches, or you can scan and automatically install all missing patches. You can target instances for patching individually, or in large groups by using Amazon EC2 tags.\n AWS does not test patches for Windows or Linux before making them available via Patch Manager - by default the patches are provided from the patch repositories maintained by the OS vendors. If any updates are installed by Patch Manager the patched instance is rebooted. Always test patches thoroughly before deploying to production environments.   KEY CONCEPT: Patch Baselines Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage.\nThe Operating Systems supported by Patch Manager may vary from those supported by the SSM Agent.\n Create a Patch Baseline    Under Instances and Nodes in the AWS Systems Manager navigation menu on the left, choose Patch Manager.\n  Click the View predefined patch baselines button in the Patch your instances section at the upper right of the console.   Click on the Create patch baseline button at the top right of the console.   On the Create patch baseline page, in the Provide patch baseline details section:\n Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline. Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches. Select Amazon Linux from the list.     In the Approval rules for operating systems section:\n Examine the options in each of the dropdown lists and ensure that Product, Classification, and Severity are set to values of All. Leave the Auto approval configuration item set to the selection Approve patches after a specified number of days and the default of 0 days. Change the value of Compliance reporting - optional to Critical.  Click on the Add rule button to create a second rule (it is possible to create up to 10 rules in the one patch baseline). For Rule 2, ensure that Product, Classification, and Severity are set to values of All. Change the value of Compliance reporting - optional to Medium. Check the box under Include nonsecurity updates to include all Amazon Linux updates when patching (not just for vulnerabilities).     If an approved patch is reported as missing, the option you choose in Compliance reporting, such as Critical or Medium, determines the severity of the compliance violation reported in System Manager Compliance.\n Expand the Patch exceptions section by clicking on the small arrow. In the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing of new releases.  For Linux operating systems, you can optionally define an alternative patch source repository. You can also add tags to a patch baseline in the Manage tags section (we will not do so in this lab). Click on the Create patch baseline button at the bottom right of the console.  Now click on the View details button at the top right of the console (in the green header bar) to go to the Patch Baselines page, to view the configuration of your custom patch baseline.   KEY CONCEPT: Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups, for example by Operating Systems (Linux or Windows), different environments (Development, Test, Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.\nYou create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers) but the key must be Patch Group.\nAn instance can only be in one patch group at a time.\n After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance.\n If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. If a patch baseline is found for that group, the system applies that patch baseline. If an instance isn\u0026rsquo;t assigned to a patch group, the system automatically uses the currently configured default patch baseline.  Assign a Patch Group   Click on the Actions in the top right of the console and select Modify patch groups.  In the Modify patch groups window in the text field for Patch groups, type Critical, click on the Add button, then click on the Close button to be returned to the Patch Baseline details screen.   KEY CONCEPT: AWS Systems Manager - Documents An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including \u0026lsquo;AWS-RunPatchBaseline\u0026rsquo;. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify.\nAll AWS-provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents. You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.\nKEY CONCEPT: AWS-RunPatchBaseline Document AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example, you can view which instances are missing patches and what those patches are.\nFor Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems.\nExamine AWS-RunPatchBaseline in Documents  To examine AWS-RunPatchBaseline in Documents:\n In the AWS Systems Manager navigation menu under Shared Resources, choose Documents. Click in the search box, select Document name prefix, and then Equals. Type AWS-Run into the text field and press Enter on your keyboard to start the search (this is case-sensitive):  Click on the document named AWS-RunPatchBaseline (click on the blue text of the document name) to view the details:  Click on the Content tab to view the configuration of the document:   KEY CONCEPT: AWS Systems Manager - Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.\nScan Your Instances with AWS-RunPatchBaseline via Run Command   Under Instances and Nodes in the AWS Systems Manager navigation menu on the left, choose Run Command. In the Run Command dashboard, you can view previously executed commands in the Command history tab, including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. Choose Run Command in the top right of the window. In the Run a command window, under Command document:  Choose the search icon and select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Select AWS-RunPatchBaseline by clicking in the small circle to the left of its name in the list.    Scroll down to Command parameters. In the Command parameters section, leave the Operation value on the default choice of Scan. In the Targets section:  Choose Specify instance tags to reveal the Specify Instance Tags fields sub-section. Under Enter a tag key, enter Environment, and under Enter a tag value, enter SSMPatchLab and click on the Add button.  Optionally the remaining Run Command features enable you to:  Specify Rate control, limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. Specify Output options to record the entire output to a pre-configured S3 bucket and optional S3 key prefix. Note Only the last 2500 characters of a command document\u0026rsquo;s output are displayed in the console. Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be pre-configured. View the command as it would appear if executed within the AWS Command Line Interface.      Click on the Run button in the bottom right of the console to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh the status list by clicking on the circular arrow at the top right of the console. Click on one of the blue names of an Instance ID in the targets list to view the Output from command execution on that instance. Click on the small arrow next to Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command. CLick on the small arrow next to Step 1 - Output again to conceal it. Click on the small arrow next to Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Click on the small arrow next to Step 2 - Output again to conceal it.  Review Initial Patch Compliance   Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu on the left, choose Compliance. On the Compliance page in the Compliance resources summary, you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details for resources:   Patch Your Instances with AWS-RunPatchBaseline via Run Command   Under Instances and Nodes in the AWS Systems Manager navigation menu, choose Run Command. Click on the Run Command button in the top right of the console. In the Run a command window, under Command document:  Click in the search text field, select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Select AWS-RunPatchBaseline by clicking in the small circle to the left of its name in the list.   In the Command parameters section, change the Operation value in the dropdown menu to Install. In the Targets section:  choose Specify instance tags. Under tag key, enter Environment and under tag value enter SSMPatchLab, then click on Add.     You could have chosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. If there are multiple pages of instances, individual selections must be made on each page.\n  Click on the small arrow next to Rate control to expand it:\n  For Concurrency, ensure that targets is selected and specify the value as 1. Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times.\n   For Error threshold, ensure that error is selected and specify the value as 1.\n    In Output options, untick the selection box next to Enable writing to an Amazon S3 bucket.\n  Click on the Run button at the bottom right of the console to execute the command and to go to its details page.   Refresh the page by clicking on the circular arrow at the top right of the console to view updated status. This step may take up to 15 minutes, so do not wait for all four targets to complete. Take a refreshment break at this point for a few minutes and then come back to the lab exercise.   Remember, if any updates are installed by Patch Manager, the patched instance may be rebooted.\n Review Patch Compliance After Patching   Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, choose Compliance. The Compliance resources summary will now show that patched systems will now have satisfied critical severity patch compliance and will now be counted as compliant resources in the Patch row.  Click on the blue number next to the green tick in the Patch row to see the newly-patched instances listed further down in the console:   KEY CONCEPT: What did we learn?  The basic concepts and functionality of the AWS Systems Manager Patch Manager feature The basic concepts and functionality of the Systems Manager Run Command feature How to setup custom Patch Baselines for Patch Manager that are aligned with your business requirements How to use the Run Command to scan for missing patches and update the Patch compliance status for reporting How to use the Run Command to install missing patches using configuration options that ensure patching will not impact availability.   Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module1/rdp/",
	"title": "6. Enable RDP Through Session Manager",
	"tags": [],
	"description": "",
	"content": "You can tunnel Remote Desktop Protocol (RDP) using Port Forwarding feature of Session Manager to get access to remote Windows instance. This can be achieved without opening inbound port 3389 (default RDP port) on remote instance.\nYou will require AWS CLI and RDP client installed on your local machine for this step and adequate access over the Internet. If you do not have required access, you may choose to skip this step.\n Prerequisites Install and configure AWS CLI and Session Manager CLI Plugin into your local machine   Install the latest version of AWS CLI\n  Install the Session Manager plugin for the AWS CLI\n  Browse to Users under the IAM console and select session-manager-demo-user from the Users   Go into the Security credentials tab and select Create access key. Note down the Access key ID and Secret Access key for step 5.   Configure the AWS CLI via the command below and enter your credentials with region as ap-southeast-2.\n aws configure    Install RDP Client on your local machine  If you do not already have the RDP Client, download and install from below   [Windows] Windows includes an RDP client by default. To verify, type mstsc at a Command Prompt window. If your computer doesn\u0026rsquo;t recognize this command, see the Windows home page and search for the download for the Microsoft Remote Desktop app.\n  [Mac OS X] Download the Microsoft Remote Desktop app from the Mac App Store.\n  [Linux] Use Remmina\n    Create a Windows OS user   Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation menu, browse to the Session Manager console and start a session to windows instance session-manager-windows-stage.\n  Type the following commands to create a new user:\n  Input password as a secure string. Enter below command which will prompt you for a password, then type a strong password and enter:\n $Password = Read-Host -AsSecureString    Create a local user:\n New-LocalUser \u0026quot;User01\u0026quot; -Password $Password    Add user to Remote Desktop Users group:\n Add-LocalGroupMember -Group \u0026quot;Remote Desktop Users\u0026quot; -Member \u0026quot;User01\u0026quot;      Click Terminate to terminate the session or enter exit and select close.\n  RDP to EC2 Instance   Browse to the EC2 Console and note instance-id for instance session-manager-windows-stage.\n  Open a terminal on your local machine and type below command to start a session to instance session-manager-windows-stage instance.\naws ssm start-session --target \u0026lt;instance-id\u0026gt; --document-name AWS-StartPortForwardingSession --parameters \u0026quot;localPortNumber=55678,portNumber=3389\u0026quot;    You should see a message indicating port 55678 has been opened for this session.   Open Microsoft Remote Desktop client and add a new remote desktop with below information.\n PC Name: localhost:55678 User Account: Provide user name and password created in earlier step Connection/Friendly Name: Session Manager RDP     Using Microsoft Remote Desktop, open remote desktop connection Session Manager RDP configured earlier for localhost:55678. You should be now connected and able to work on remote instance over RDP.   Press Control+C on terminal to terminate the session.   Congratulations! You have now completed this workshop!\n  Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module2/configlabsetup/",
	"title": "6. AWS Config Lab Setup",
	"tags": [],
	"description": "",
	"content": "Create a Trail in CloudTrail    Open the CloudTrail service console.\n  Click on Trails in the navigation menu on the left.\n  Click on the Create Trail button, to create a trail for this lab.   Apply the following settings and create the trail\n Trail name: management-tools-workshop Apply trail to all regions: Yes Read/Write Events: All  Data Events:  Check the box next to Select all S3 Buckets in your account   Create a new S3 Bucket:Yes   S3 bucket: security-workshop-(today\u0026rsquo;s date)-(yourmobilenumber)- Example: security-workshop-10062020-123456789.   Note down your bucket name as you will need it when deploying lab resources later.\n      We are using mobile phone number at the end to ensure that we create a unique bucket per user. For more information on Bucket Restrictions and Limitations click here\n Click on the Create button at the bottom right of the console. You will now see the new trail listed.  We will now configure the new trail to send logs to CloudWatch to make searching logs easier.  Click on the trail name. Scroll down to the CloudWatch Logs section and click on Configure.  In the New or existing log group section, type the following name SecurityWorkshop/CloudTrail, then click on the Continue button.  On the next screen click on the Allow button. This grants CloudTrail the ability to assume a role to write to CloudWatch Logs.     Congratulations! You have now created a CloudTrail trail and added a CloudWatch log for the API activity in your AWS Account.\nTurn on AWS Config  AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.\n Open the AWS Config console. Click on the Get Started button.   Leave all settings on their default choices and click on the Next button – This will create an S3 bucket, a role for the Config service, and begin recording configuration history for resources in the account:  On the AWS Config Rules screen click on the Next button to skip selecting any AWS Config Rules for the moment. On the Review screen click on the Confirm button.  Congratulations! AWS Config is now turned on in your account and is tracking configuration history.\nDeploy Components for Lab    Click here To deploy lab resources into your account\n  On the Create stack page leave all settings on their defaults and click on the Next button at the bottom right of the console.\n  In the Parameters section:\n for BucketName enter the name of the S3 bucket that you created earlier during the CloudTrail configuration setup. Leave the TopicEmail field as it is.    Click on the Next button to continue.\n  On the Configure stack options screen click on the Next button to continue.\n  On the Review screen scroll to the bottom. Tick the box next to I acknowledge that AWS CloudFormation might create IAM resources, then click on the Create stack button:   Wait for the stack to deploy. This will typically take up to five minutes to complete.\n  The AWS Config Lab environment has now been created. Please now move on to Part 7 of the lab.\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module2/config/",
	"title": "7. AWS Config: Compliance as Code",
	"tags": [],
	"description": "",
	"content": "Create a Config Rule to alert on whether the SSM Agent is working In this step we will add a config rule to our environment using an AWS Managed rule that will evaluate if EC2 instances in the account have a working AWS Systems Manager Agent.\n Open the AWS Config console. Click on Rules in the navigation menu at the left of the console. Click on the Add rule button:  On the Add Rule screen type ec2-instance-managed-by-systems-manager in the Find by rule name search field. Click on the displayed rule box:  In the Trigger section take note of the Trigger type (When configuration changes):  Leave all other settings on their defaults and click on the Save button.   You can create Config Rules to monitor a number of items within your infrastructure. Besides utilizing AWS managed Config rules you can also create custom rules using Lambda Functions. Located here in Github are some sample Config rules you can create and implement using AWS Lambda.\n Test Config Rule by Deploying an EC2 Instance  To test the AWS Config rule we will now deploy a new EC2 instance.\n Open the EC2 console. In the navigation menu on the left, click on Instances, under the INSTANCES heading:  Click on the Launch Instance button:  Choose the first Operating System choice in the list - Amazon Linux 2 AMI (HVM) - by clicking on the Select button next to it:  On the Step 2: Choose an Instance Type screen tick the box next to t3.large, then click on the button Review and Launch:  On the Review Instance Launch screen click on the Launch button. Select the SSMLabKeys key pair and tick the box to acknowledge the warning. Click on the Launch Instances button:   The EC2 instance will now be created for you.\nClick on the view Instances button at the bottom right of the console. Look for the instance that has no name in the Name column (this will be the one you just created). Make a copy of the Instance ID:  Open the AWS Config console.  Click on Rules. The rule you deployed earlier will be shown. click on the rule name (ec2-instance-managed-by-systems-manager):  If the EC2 instance has been deployed and running for several minutes the rule will have updated automatically to add the new EC2 resource that is non-compliant to the list. Look for the Instance ID you noted down in step 8 (If it has not yet updated, click on the Re-evaluate button in the top left of the console, then refresh the web page until the rule flags the EC2 instance as Noncompliant):     Remediate Non-compliant Resources  Now we will remediate the non-compliant resource:\n  Open the CloudFormation console.\n  Click on the blue stack name ConfigLabStack:   Click on the Outputs tab.\n  Look for the Key called EC2SSMRoleName and copy the Value next to it to a text document (it will look similar to this - ConfigLabStack-SSMConfigEC2LabRole-, with some numbers and characters at the end):   Open the AWS Config console.\n  Click on Rules in the navigation menu on the left of the console.\n  Click on the blue rule name ec2-instance-managed-by-systems-manager.\n  Click on the Edit button to edit the ec2-instance-managed-by-systems-manager Config rule:   In the Choose Remediation Action section do the following:\n Remediation action: choose AWS-AttachIAMToInstance Click on the dropdown selection for Resource ID parameter and choose InstanceId (this will pass the non-compliant Instance ID to the remediation action) In the Parameters section paste the value you copied in step 4 into the field next to RoleName     Click on the Save button.\n  Click on the AWS Config rule name ec2-instance-managed-by-systems-manager and look at the non-compliant resource. Click on the selection box next to the EC2 instance you deployed earlier and then click on Remediate:   Open the AWS Systems Manager console and click on Automation in the left hand menu. You should see an Automation execution task listed that will attach an IAM role to the EC2 instance:   Restart the EC2 instance to quicken the process:\n Go to the EC2 Console Select the box next to your t3.large instance Click on the Actions button and choose Instance State, then choose Reboot. Click on the button Yes, Reboot.    Go back to the AWS Systems Manager console, and click on Managed Instances in the left hand menu. The EC2 instance should now be shown as a managed instance in the list (the deployed instance will have a blank Name field in the list). This step may take up to two minutes.\n  Go back to the AWS Config console and click on Rules.\n  Click on the rule name ec2-instance-managed-by-systems-manager.\n  The EC2 instance will no longer be displayed in the non-compliant list. Change the Compliance Status selection to Compliant and observe that the EC2 instance is now listed there.\n  KEY CONCEPT: What did we learn?  How to create an AWS Config Rule to evaluate if instances are managed by SSM Importance of having the right IAM Role assigned for the instance to report to SSM. How to use AWS Systems Manager Automation documents to remediate non-compliant Instances.  Congratulations! You\u0026rsquo;ve now completed Lab 2!\n Stuck? Watch this This video has no audio\n Your browser doesn't support video.  "
},
{
	"uri": "/workshops/module1/",
	"title": "Lab 1: Eliminate Bastion Hosts with Systems Manager",
	"tags": [],
	"description": "",
	"content": "Eliminate bastion hosts with AWS Systems Manager Session Manager AWS Systems Manager Session Manager improves your security posture for instance access with a browser-based and CLI interactive shell experience that requires no open inbound ports or access/jump servers, and enables customer key encryption using AWS KMS. With IAM access control, sessions audited using AWS CloudTrail, and session output logged to Amazon S3 or Amazon CloudWatch Logs, Session Manager makes it easy to control and secure access to instances in operational scenarios while complying with corporate policies and security best practices. Dive deep into AWS Systems Manager Session Manager to see how it works for Linux or Windows instances, in the cloud, or on premises.\nLab Overview Your browser doesn't support video.   Speakers: Sara Gray (AWS), Fahad Tariq (CMD Solutions)\n  In this video, you’ll also hear from our partner CMD Solutions highlighting a real-world example of the deployment of the AWS services you’ll use in this demo. For more information, please visit here\n Lab Steps  1. Overview and Setup     2. Evaluate Session Manager Configuration     3. Configure Systems Manager Session Manager     4. Use Port Forwarding For Web Redirection     5. Enable SSH Through Session Manager     6. Enable RDP Through Session Manager     Click here to get started!\n"
},
{
	"uri": "/workshops/module2/",
	"title": "Lab 2: Security through Good Governance",
	"tags": [],
	"description": "",
	"content": "In this session, you will leverage Systems Manager and AWS Config to enforce governance across your AWS resources. You will collect inventory from your instances, automate patch management, ensuring consistency across your instances and automating compliance enforcement.\nLab Overview Your browser doesn't support video.   Speakers: Michael Stringer (AWS), Mark Promnitz (Itoc)\n  In this video, you’ll also hear from our partner Itoc highlighting a real-world example of the deployment of the AWS services you’ll use in this demo. For more information, please visit here\n Lab Steps  1. AWS Systems Manager Lab Setup     2. AWS Systems Manager: Operations as Code     3. AWS Systems Manager: Inventory     4. AWS Systems Manager: State Manager     5. AWS Systems Manager: Patch Manager     6. AWS Config Lab Setup     7. AWS Config: Compliance as Code     Click here to get started!\n"
},
{
	"uri": "/workshops/module3/",
	"title": "Lab 3: Protecting Workloads from the Instance to the Edge",
	"tags": [],
	"description": "",
	"content": "In this session, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them.\nLab Overview Your browser doesn't support video.   Speakers: Sumit Patel (AWS), Simon Morse (Versent)\n  In this video, you’ll also hear from our partner Versent highlighting a real-world example of the deployment of the AWS services you’ll use in this demo. For more information, please visit here\n Lab Steps  1. Scenario     2. Perimeter Layer - Assess     3. Perimeter Layer - Remediate     4. Perimeter Layer - Verify     5. Host Layer - Assess     6. Host Layer - Remediate     7. Host Layer - Verify     Click here to get started!\n"
},
{
	"uri": "/workshops/module4/",
	"title": "Lab 4: AWS Secrets Manager with Amazon RDS and AWS Fargate",
	"tags": [],
	"description": "",
	"content": "This Secrets Manager lab guides you through the use of AWS Secrets Manager with Amazon RDS and AWS Fargate . In the first phase you will access the RDS database with Secrets Manager. You will then use Secrets Manager to rotate the data base password. You will then use Secrets Manager to access the database again to show that you can continue to access the data base after the rotation.\nIn the second phase of the lab, you will extend your use of Secrets Manager into an AWS Fargate container. You will create an Amazon ECS task definition to pass secrets to the Fargate container and then launch the Fargate container. You will then SSH into the container to show that the secret was passed to the container and that you can access the RDS data base.\nLab Overview Your browser doesn't support video.   Speakers: Chamandeep Singh (AWS), Julian Hickie (Cloudten)\n  In this video, you’ll also hear from our partner Cloudten highlighting a real-world example of the deployment of the AWS services you’ll use in this demo. For more information, please visit here\n Lab Steps  1. Architecture     2. RDS with Secrets Manager     3. Fargate with Secrets Manager     Click here to get started!\n"
},
{
	"uri": "/training/",
	"title": "学習用リソース",
	"tags": [],
	"description": "",
	"content": "Security on AWS Training Courses    Course Description Course Length     Getting Started with AWS Security, Identity, and Compliance (Free) This course provides an overview of AWS security technology, use cases, benefits, and services. The course introduces various services in the AWS Security, Identity, and Compliance service category. By the end of this course, you will gain an understanding about the importance of security in the cloud and be able to identify AWS services that you can use to secure your data. 3 hours   AWS Security Fundamentals (Free) In this self-paced course, you will learn fundamental AWS cloud security concepts, including AWS access control, data encryption methods, and how network access to your AWS infrastructure can be secured. We will address your security responsibility in the AWS Cloud and the different security-oriented services available. 2 hours   AWS Security Essentials This course covers fundamental AWS cloud security concepts, including AWS access control, data encryption methods, and how network access to your AWS infrastructure can be secured. Based on the AWS Shared Security Model, you learn where you are responsible for implementing security in the AWS Cloud and what security-oriented services are available to you and why and how the security services can help meet the security needs of your organization. 1 Day   Security Engineering on AWS This course demonstrates how to efficiently use AWS security services to stay secure in the AWS Cloud. The course focuses on the security practices that AWS recommends for enhancing the security of your data and systems in the cloud. It highlights the security features of AWS key services including compute, storage, networking, and database services. You will also learn how to leverage AWS services and tools for automation, continuous monitoring and logging, and responding to security incidents. 3 Days    "
},
{
	"uri": "/links/",
	"title": "リンク",
	"tags": [],
	"description": "",
	"content": "Workshop Cloudformation Templates If you prefer to run the labs in your own AWS account, launch the relevant CloudFormation stack below using the \u0026ldquo;Deploy to AWS\u0026rdquo; links below. This will automatically take you to the console to run the template. In order to complete these labs, you\u0026rsquo;ll need a valid, usable AWS Account. Use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for. We strongly recommend that you use a non-production AWS account for this workshop such as a training, sandbox or personal account. If multiple participants are sharing a single account you should use unique names for the stack set and resources created in the console.\nInstructions on how to deploy the templates below are in the respective labs.\n    Lab Template Region     Lab 1 - Eliminate bastion hosts with AWS Systems Manager Session Manager  AP Southeast 2 (Sydney)   Lab 2 - Security through Good Governance (1 - Systems Manager)  AP Southeast 2 (Sydney)   Lab 2 - Security through Good Governance (2 - Config)  AP Southeast 2 (Sydney)   Lab 3 - Protecting Workloads from the Instance to the Edge  AP Southeast 2 (Sydney)   Lab 4 - AWS Secrets Manager with Amazon RDS and AWS Fargate  AP Southeast 2 (Sydney)    AWS Security Partners Additional Info    Partner Additional Info     Cloudten    Versent    CMD Solutions     "
},
{
	"uri": "/survey/",
	"title": "アンケート",
	"tags": [],
	"description": "",
	"content": "Please complete the survey to help the AWS team to iterate and make this workshop even better.\nAWS Cloud Security Virtual Event Survey "
},
{
	"uri": "/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " How to request your hash for EventEngine and open the AWS Console? How to ask for help? I am Stuck on a step in the labs? I need more time to do the labs The Website or AWS Console is not working properly on my browser I am unable to connect as instructed in the labs I am unable to connect via SSH or RDP as instructed in Lab 1, Step 6  How to request your hash for EventEngine and open the AWS Console?  Request hash code by asking a question in GotoWebinar. Expand the Questions pane within GotoWebinar and request a hash.  Open EventEngine  Enter hash  Open AWS Console  Now you can begin the labs!  How to ask for help  Within your GotoWebinar panel, expand the Questions pane and ask a question   I am Stuck on a step in the labs   Watch the I am Stuck video at the end of the page on the step   If that still does not help, ask one of our moderators for help\n  I need more time to do the labs   Choose 1 or 2 labs that you\u0026rsquo;d like to do with AWS staff on standby.\n  The EventEngine labs will be available till midnight AEST, you can continue to work on the labs without AWS staff support.\n  You can also deploy the Cloudformation templates found in the Additional Links into your own AWS accounts.\n  The Website or AWS Console is not working properly on my browser   Download and install the latest versions of Google Chrome, Mozilla Firefox or Microsoft Internet Explorer 11.\n  If you are using a corporate machine behind a VPN, you may wish to test it outside of the VPN.\n  I am unable to connect as instructed in the labs   Download and install the latest versions of Google Chrome, Mozilla Firefox or Microsoft Internet Explorer 11.\n  If you are using a corporate machine behind a VPN, you may wish to test it outside of the VPN.\n  If you seat behind restricted access to the Internet, you may not be able to complete all labs.\n  Lab 1, Step 6 - I am unable to get this working or connecting via SSH or RDP as instructed   You require local admin access for this particular step. If you do not have that, you may wish to skip this step.\n  Your internet access may be restricted preventing connectivity.\n  "
},
{
	"uri": "/ondemandtracks/advanced_container_security/",
	"title": "Advanced container security",
	"tags": [],
	"description": "",
	"content": "Learn how to leverage the identity and authorisation, network security and secrets management features of the wider AWS platform for their containers, including Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Container Service for Kubernetes (Amazon EKS). We also discuss best practices for the security of your container images such as scanning them for known vulnerabilities.\nYour browser doesn't support video.\r\r **Speakers: Jason Umiker, Solutions Architect, Amazon Web Services **\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/ondemandtracks/cloud_security_multi_account/",
	"title": "Cloud security for everyone: Multi-account strategy",
	"tags": [],
	"description": "",
	"content": "The cloud enables every business to have enterprise-grade security. Leveraging multiple accounts is an essential security pattern, even in small teams without dedicated security personnel. In this session, we dive deep into the accounts and learn how to configure them. Attendees are expected to have an understanding of the shared responsibility model and IAM. Note: The linked content in the video is now available at https://bit.ly/3gDSGWA\nYour browser doesn't support video.\r\r Speakers: Byron Pogson, Solutions Architect, Amazon Web Services\n "
},
{
	"uri": "/ondemandtracks/cloud-enabled_security_origin/",
	"title": "Cloud-enabled security evolution with Origin Energy",
	"tags": [],
	"description": "",
	"content": "Moving your business to the cloud is a once-in-a-generation opportunity to significantly evolve your security capability and culture. Origin Energy, Australia’s largest energy retailer, started its cloud journey a few years ago. In this session, Origin Energy’s chief security officer and its security lead for cloud discuss their experience transforming a largely outsourced security capability into an in-house, business-aligned team. Learn how the company builds and runs cloud-native security at scale, at low cost, and with improved security.\nYour browser doesn't support video.\r\r Speakers: Christoph Strizik, Chief Information Security Officer, Origin Energy, and Glenn Bolton, Security Lead for Cloud, Origin Energy\n "
},
{
	"uri": "/ondemandtracks/federated_access_made_simple/",
	"title": "Federated access and authorisation made simple",
	"tags": [],
	"description": "",
	"content": "In this session, learn how to implement attribute-based access control with role-based access control. We discuss how you can use this strategy to ensure that people have the right access to the things they need in their role, and we show you how to simplify their IAM policies in the process. Also learn how automation can deliver the consistency of access and authorisation, and how you can apply this to your environment.\nYour browser doesn't support video.\r\r Speakers: Louay Shaat, Senior Solutions Architect, Amazon Web Services\n "
},
{
	"uri": "/ondemandtracks/secure_real-time_tracking_afl/",
	"title": "How AFL secures real-time player tracking with encryption",
	"tags": [],
	"description": "",
	"content": "Through the sharing of real-time data and insights about the prevailing game and players, fan engagement in sports has been revolutionised. However, the sensitivity, influence, and impact of such data, as determined by various entities in the sports ecosystem, is critical. In this session, discover how a highly secure application has been designed and implemented not only to appease the various sporting entities, but also to ensur\nYour browser doesn't support video.\r\r Speakers: Sri Nadendla, Enterprise Solutions Architect, Amazon Web Services, and Ralph Stone, Lead Architect, Media Applications, Telstra\n "
},
{
	"uri": "/ondemandtracks/secops_in_your_organization/",
	"title": "How to put SecOps to work in your organisation",
	"tags": [],
	"description": "",
	"content": "Open Universities Australia (OUA) migrated their core business systems to AWS in 2014 and have continued to optimise their environment on AWS. Leveraging AWS tools, OUA have automated responses to security events, limiting intervention of engineering staff, and enabling secure self-service tasks to simplify access to secure systems. In this session, OUA covers what worked, what didn\u0026rsquo;t, and what they learned along the way.\nYour browser doesn't support video.\r\r Speakers: Sam Annis-Brown, Solutions Architect, Amazon Web Services\n "
},
{
	"uri": "/ondemandtracks/neoband_in_th_cloud_xinja/",
	"title": "How Xinja built a neobank on the cloud",
	"tags": [],
	"description": "",
	"content": "Xinja is a 100-percent digital cloud-based neobank composed of a microservices architecture built with Kubernetes and Apache Kafka on AWS and hooked into many modern, cloud-based banking, payment, and channel platforms. This session focuses on how Xinja built its technology stack to exceed stringent security, risk, and resiliency requirements. Learn how it established a contemporary cloud network foundation, delivered transaction and deposit accounts with debit card payment capability, and integrated Apple Pay and Google Pay (including PCI DSS compliance). Additionally, hear how Xinja created multiple on-demand data pipelines and worked with APRA to secure its banking license and revolutionise its customers’ banking experience.\nYour browser doesn't support video.\r\r Speakers: Maria Sokolova, Solutions Architect, Amazon Web Services, and Greg Steel, CIO, Architect, and Co-Founder, Xinja\n "
},
{
	"uri": "/ondemandtracks/iam-best-practices/",
	"title": "IAM: Best practices for managing identity with AWS",
	"tags": [],
	"description": "",
	"content": "AWS Identity and Access Management (IAM) enables you to securely manage access to AWS services and resources. Using IAM, you can create and manage AWS users and groups, as well as use permissions to allow and deny their access to AWS resources. In this session, you learn best practices for managing user identity and permissions with AWS. We examine role-based access control (RBAC) and attribute-based access control (ABAC) models to ensure that people have the right access to what they need to perform their roles.\nYour browser doesn't support video.\r\r Speakers: Kimberly Chow, Solutions Architect, Amazon Web Services; Amit Gupta, CTO, Canopy\n "
},
{
	"uri": "/ondemandtracks/intro-to-aws-sec/",
	"title": "Introduction to AWS Security",
	"tags": [],
	"description": "",
	"content": "Ensuring security and compliance is a shared responsibility between AWS and the customer. In this session, we introduce the AWS Shared Responsibility Model along with key security services that allow you to build security controls that are aligned to the NIST Cybersecurity Framework categories: identify, protect, detect, respond, and recover. You also hear from a financial institution in Singapore about how they are developing a cloud security strategy that allows innovation within defined risk guardrails.\nYour browser doesn't support video.\r\r Speakers: Myles Hosford, Principal, Enterprise Security, Amazon Web Services\n "
},
{
	"uri": "/ondemandtracks/sec-best-practices-war-way/",
	"title": "Security best practices: The Well-Architected way",
	"tags": [],
	"description": "",
	"content": "As you continually evolve your use of AWS, it’s important to consider ways to improve your security posture and take advantage of new security services and features. In this session, you explore architectural patterns for meeting common challenges, learn about service limits, and hear some tips and tricks, as well as learn ways to continually evaluate your architecture against best practices. Automation and tools are featured throughout, and we also include code giveaways!\n   Speakers: Ben Potter, Security Lead, Well-Architected, Amazon Web Services\n "
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/ondemandtracks/security_fundamentals/",
	"title": "The fundamentals of AWS Security",
	"tags": [],
	"description": "",
	"content": "AWS offers an ever-growing landscape of services designed for a wide range of workloads in the cloud. But how do you secure all those different types of workloads? This session, intended for security-minded builders, introduces the fundamental AWS security building blocks that can be simply, easily, and authoritatively applied to anything you build on AWS.\nYour browser doesn't support video.\r\r Speakers: Pierre Liddle, Principal Security Specialist Solutions Architect, Amazon Web Services\n "
}]